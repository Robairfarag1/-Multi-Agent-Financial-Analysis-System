{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c669380d",
   "metadata": {},
   "source": [
    "# Multi-Modal Financial Analysis Agent: Final Submission\n",
    "\n",
    "**Course:** AAI 520 - Final Project\n",
    "\n",
    "This notebook implements the final version of a multi-modal AI system that performs comparative analysis on multiple stocks using market, macroeconomic, and news sentiment data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac333a",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "This cell installs the required `vaderSentiment` library, imports all necessary packages, and sets the API keys.**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c2cbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Uncomment and run the following line if you haven't installed the required packages yet\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Uncomment and run the following line if you haven't installed the required packages yet'''\n",
    "#!py -m pip install openai python-dotenv yfinance pydantic requests vaderSentiment google-generativeai tabulate seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaeaa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "import os, json, time, argparse, datetime as dt\n",
    "import openai\n",
    "import hashlib, hmac, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple, Callable, Iterable\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "import textwrap\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# tqdm (nice progress bars)\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d56012",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Enviroment set with .env file as follows:\n",
    "\n",
    "```env\n",
    "# Economic data from the Federal Reserve\n",
    "FRED_API_KEY=\"YOUR_FRED_API_KEY\"\n",
    "\n",
    "# Stock market and financial data\n",
    "POLYGON_API_KEY=\"YOUR_POLYGON_API_KEY\"\n",
    "FINNHUB_API_KEY=\"YOUR_FINNHUB_API_KEY\"\n",
    "\n",
    "# Real-time news articles\n",
    "NEWS_API_KEY=\"YOUR_NEWS_API_KEY\"\n",
    "\n",
    "# For the AI agent's \"brain\"\n",
    "OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "# Identification for accessing SEC's EDGAR database\n",
    "SEC_USER_AGENT=\"Your Name you@example.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a467d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API keys from environment variables\n",
    "FRED_KEY        = os.getenv(\"FRED_API_KEY\")\n",
    "NEWS_KEY        = os.getenv(\"NEWS_API_KEY\")\n",
    "FINNHUB_KEY     = os.getenv(\"FINNHUB_API_KEY\")\n",
    "POLYGON_KEY     = os.getenv(\"POLYGON_API_KEY\")\n",
    "OPENAI_KEY      = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_KEY      = os.getenv(\"GOOGLE_API_KEY\")\n",
    "SEC_USER_AGENT  = os.getenv(\"SEC_USER_AGENT\")\n",
    "\n",
    "# Configuration\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"    \n",
    "GEMINI_MODEL = \"gemini-2.5-flash\"    \n",
    "NEWS_STORE   = \"news_store.parquet\" \n",
    "CACHE_DIR    = \".cache\"             \n",
    "\n",
    "# Ensure cache directory exists\n",
    "Path(CACHE_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfe713",
   "metadata": {},
   "source": [
    "### Test LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a342a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking LLM Availability ---\n",
      "âœ… Google Gemini: OK\n",
      "âœ… OpenAI GPT: OK\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "\n",
    "def check_llm_availability():\n",
    "    \"\"\"\n",
    "    Checks the availability and functionality of configured LLM APIs.\n",
    "    This function is designed to be run directly in a notebook cell.\n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"--- Checking LLM Availability ---\")\n",
    "    # --- Test 1: Google Gemini ---\n",
    "    try:\n",
    "        assert GEMINI_KEY, \"GOOGLE_API_KEY is not set in your environment.\"\n",
    "        genai.configure(api_key=GEMINI_KEY)\n",
    "        \n",
    "        # Using a reliable and recent model\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "        prompt = 'Return ONLY this JSON: {\"ok\": true}'\n",
    "        response = model.generate_content(prompt, generation_config={\"temperature\": 0})\n",
    "        if \"ok\" in response.text:\n",
    "            print(\"âœ… Google Gemini: OK\")\n",
    "        else:\n",
    "            print(f\"âŒ Google Gemini: Unexpected response -> {response.text.strip()}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Google Gemini: FAILED - {e}\")\n",
    "\n",
    "    # --- Test 2: OpenAI GPT ---\n",
    "    try:\n",
    "        assert OPENAI_KEY, \"OPENAI_API_KEY is not set in your environment.\"\n",
    "        client = OpenAI(api_key=OPENAI_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model = OPENAI_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Reply with exactly: OK\"}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        if output_text == \"OK\":\n",
    "            print(\"âœ… OpenAI GPT: OK\")\n",
    "        else:\n",
    "            print(f\"âŒ OpenAI GPT: Unexpected response -> {output_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ OpenAI GPT: FAILED - {e}\")\n",
    "\n",
    "# Check now\n",
    "check_llm_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bcdbe",
   "metadata": {},
   "source": [
    "## Data Tools and Helper Classes/Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd155283",
   "metadata": {},
   "source": [
    "Lightweight Utils (DiskCache + stable id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf77c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Persistent Memory (tiny JSON)\n",
    "# -------------------------------\n",
    "class MemoryStore:\n",
    "    \"\"\"\n",
    "    A simple JSON-based memory store for the agent to learn across runs.\n",
    "    Saves and retrieves brief notes about symbols.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = \".agent_memory.json\"):\n",
    "        self.path = path\n",
    "        if not os.path.exists(self.path):\n",
    "            with open(self.path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"symbols\": {}}, f)\n",
    "\n",
    "    def _load(self) -> Dict[str, Any]:\n",
    "        with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _save(self, data: Dict[str, Any]):\n",
    "        with open(self.path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def add_note(self, symbol: str, note: str):\n",
    "        \"\"\"Adds a new memory note for a given stock symbol.\"\"\"\n",
    "        data = self._load()\n",
    "        symbols = data.setdefault(\"symbols\", {})\n",
    "        note_list = symbols.setdefault(symbol.upper(), [])\n",
    "        \n",
    "        timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "        note_list.append({\"ts\": timestamp, \"note\": note})\n",
    "        self._save(data)\n",
    "        print(f\"   - Memory added for {symbol.upper()}\")\n",
    "\n",
    "    def get_notes(self, symbol: str, last_n: int = 5) -> str:\n",
    "        \"\"\"Retrieves the last N notes for a symbol as a single string.\"\"\"\n",
    "        data = self._load()\n",
    "        notes = data.get(\"symbols\", {}).get(symbol.upper(), [])\n",
    "        if not notes:\n",
    "            return \"No past notes found for this symbol.\"\n",
    "        \n",
    "        formatted_notes = [f\"- {n['ts']}: {n['note']}\" for n in notes[-last_n:]]\n",
    "        return \"### Past Analysis Notes:\\n\" + \"\\n\".join(formatted_notes)\n",
    "    \n",
    "# -------------------------------\n",
    "# Disk Cache (parquet files)\n",
    "# -------------------------------\n",
    "class DiskCache:\n",
    "    # ... (This class is correct, no changes needed) ...\n",
    "    def __init__(self, cache_dir: str, ttl_seconds: int):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    def _cache_path(self, key: str) -> str:\n",
    "        h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f\"{h}.parquet\")\n",
    "    def get(self, key: str) -> pd.DataFrame | None:\n",
    "        path = self._cache_path(key)\n",
    "        if not os.path.exists(path): return None\n",
    "        if (time.time() - os.path.getmtime(path)) > self.ttl_seconds: return None\n",
    "        try: return pd.read_parquet(path)\n",
    "        except Exception: return None\n",
    "    def set(self, key: str, df: pd.DataFrame):\n",
    "        path = self._cache_path(key)\n",
    "        df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10064f8",
   "metadata": {},
   "source": [
    "### Economic Data From FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1df7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EconomicDataTool:\n",
    "    \"\"\"\n",
    "    A tool to fetch economic data series from the FRED API.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "\n",
    "    def __init__(self, cache_dir: str = \".cache/fred\", ttl_seconds: int = 12 * 3600):\n",
    "        self.api_key = os.getenv(\"FRED_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            print(\"âš ï¸ FRED_API_KEY not set. The EconomicDataTool will be disabled.\")\n",
    "        \n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def _cache_path(self, key: str) -> str:\n",
    "        h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f\"{h}.parquet\")\n",
    "\n",
    "    def _read_cache(self, key: str) -> pd.DataFrame | None:\n",
    "        path = self._cache_path(key)\n",
    "        if not os.path.exists(path): return None\n",
    "        if (time.time() - os.path.getmtime(path)) > self.ttl_seconds: return None\n",
    "        try: return pd.read_parquet(path)\n",
    "        except Exception: return None\n",
    "\n",
    "    def _write_cache(self, key: str, df: pd.DataFrame):\n",
    "        path = self._cache_path(key)\n",
    "        df.to_parquet(path, index=False)\n",
    "\n",
    "    def get_series(self, series_ids: list[str], start_date: str = \"2020-01-01\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches one or more economic data series from FRED and merges them.\n",
    "        \n",
    "        Common Series IDs:\n",
    "        - GDP: Real Gross Domestic Product\n",
    "        - CPIAUCSL: Consumer Price Index (Inflation)\n",
    "        - UNRATE: Unemployment Rate\n",
    "        - FEDFUNDS: Federal Funds Effective Rate\n",
    "        \"\"\"\n",
    "        if not self.api_key:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Create a stable cache key from the sorted list of series\n",
    "        sorted_ids = sorted(series_ids)\n",
    "        cache_key = f\"fred::{'&'.join(sorted_ids)}::{start_date}\"\n",
    "        \n",
    "        cached_df = self._read_cache(cache_key)\n",
    "        if cached_df is not None:\n",
    "            return cached_df\n",
    "\n",
    "        all_series_dfs = []\n",
    "        for series_id in sorted_ids:\n",
    "            params = {\n",
    "                \"series_id\": series_id,\n",
    "                \"api_key\": self.api_key,\n",
    "                \"file_type\": \"json\",\n",
    "                \"observation_start\": start_date,\n",
    "            }\n",
    "            try:\n",
    "                response = requests.get(self.BASE_URL, params=params, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                data = response.json().get(\"observations\", [])\n",
    "                \n",
    "                if not data:\n",
    "                    print(f\"No data returned for FRED series: {series_id}\")\n",
    "                    continue\n",
    "\n",
    "                df = pd.DataFrame(data)\n",
    "                df = df[[\"date\", \"value\"]]\n",
    "                df = df.rename(columns={\"value\": series_id})\n",
    "                \n",
    "                # Clean the data\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                # FRED uses '.' for missing values\n",
    "                df[series_id] = pd.to_numeric(df[series_id], errors='coerce')\n",
    "                \n",
    "                all_series_dfs.append(df)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to fetch FRED series {series_id}: {e}\")\n",
    "        \n",
    "        if not all_series_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Merge all individual series DataFrames into one\n",
    "        merged_df = reduce(lambda left, right: pd.merge(left, right, on='date', how='outer'), all_series_dfs)\n",
    "        merged_df = merged_df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        self._write_cache(cache_key, merged_df)\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32435dbf",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ce9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ’¨ Running Smoke Test for EconomicDataTool ---\n",
      "Fetching series: GDP, CPIAUCSL, UNRATE...\n",
      "\n",
      "âœ… Test PASSED: Successfully fetched 68 data points.\n",
      "--- Sample of Fetched Economic Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>GDP</th>\n",
       "      <th>UNRATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>323.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>322.132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>321.500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>320.580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>320.321</td>\n",
       "      <td>30485.729</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  CPIAUCSL        GDP  UNRATE\n",
       "0 2025-08-01   323.364        NaN     4.3\n",
       "1 2025-07-01   322.132        NaN     4.2\n",
       "2 2025-06-01   321.500        NaN     4.1\n",
       "3 2025-05-01   320.580        NaN     4.2\n",
       "4 2025-04-01   320.321  30485.729     4.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_economic_data_tool_smoke_test():\n",
    "    \"\"\"\n",
    "    A simple test to verify the EconomicDataTool is working correctly.\n",
    "    \"\"\"\n",
    "    print(\"--- ðŸ’¨ Running Smoke Test for EconomicDataTool ---\")\n",
    "    \n",
    "    # Ensure environment variables are loaded (especially FRED_API_KEY)\n",
    "    load_dotenv()\n",
    "    \n",
    "    # 1. Instantiate the tool\n",
    "    tool = EconomicDataTool()\n",
    "    \n",
    "    # 2. Check if the API key is available before proceeding\n",
    "    if not tool.api_key:\n",
    "        print(\"âŒ Test SKIPPED: FRED_API_KEY is not set in your environment.\")\n",
    "        return\n",
    "\n",
    "    # 3. Define a few common and reliable FRED series IDs to fetch\n",
    "    series_to_fetch = {\n",
    "        \"GDP\": \"Real Gross Domestic Product\",\n",
    "        \"CPIAUCSL\": \"Consumer Price Index (Inflation)\",\n",
    "        \"UNRATE\": \"Unemployment Rate\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching series: {', '.join(series_to_fetch.keys())}...\")\n",
    "    \n",
    "    # 4. Call the tool's main method\n",
    "    df = tool.get_series(series_ids=list(series_to_fetch.keys()))\n",
    "    \n",
    "    # 5. Verify the output\n",
    "    if df is not None and not df.empty:\n",
    "        print(f\"\\nâœ… Test PASSED: Successfully fetched {len(df)} data points.\")\n",
    "        print(\"--- Sample of Fetched Economic Data ---\")\n",
    "        display(df.head())\n",
    "    else:\n",
    "        print(\"\\nâŒ Test FAILED: The tool returned an empty DataFrame.\")\n",
    "        print(\"   Please check your FRED_API_KEY and network connection.\")\n",
    "\n",
    "# --- Execute the smoke test ---\n",
    "run_economic_data_tool_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878d3f4",
   "metadata": {},
   "source": [
    "### Market Data From Yahoo Finance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb65c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataTool:\n",
    "    \"\"\"\n",
    "    Market data access + light feature engineering (optional).\n",
    "    - Standardized schema: ['date','open','high','low','close','volume']\n",
    "    - Intraday support (1m/2m/5m/15m/30m/60m/90m/1h)\n",
    "    - Simple on-disk caching with TTL\n",
    "    - Batch fetch for multiple tickers -> long format with a 'ticker' column\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \".cache/yfinance\",\n",
    "        ttl_seconds: int = 3600,\n",
    "        max_retries: int = 2,\n",
    "        pause_between_retries: float = 0.7\n",
    "    ):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.max_retries = max_retries\n",
    "        self.pause_between_retries = pause_between_retries\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Core helpers\n",
    "    # ---------------------------\n",
    "    def _cache_path(self, key: str) -> str:\n",
    "        h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f\"{h}.parquet\")\n",
    "\n",
    "    def _read_cache(self, key: str) -> Optional[pd.DataFrame]:\n",
    "        path = self._cache_path(key)\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "        if (time.time() - os.path.getmtime(path)) > self.ttl_seconds:\n",
    "            return None\n",
    "        try:\n",
    "            return pd.read_parquet(path)\n",
    "        except Exception:\n",
    "            # Fallback to CSV if parquet fails (rare)\n",
    "            alt = path.replace(\".parquet\", \".csv\")\n",
    "            if os.path.exists(alt):\n",
    "                try:\n",
    "                    return pd.read_csv(alt, parse_dates=[\"date\"])\n",
    "                except Exception:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "    def _write_cache(self, key: str, df: pd.DataFrame) -> None:\n",
    "        path = self._cache_path(key)\n",
    "        try:\n",
    "            df.to_parquet(path, index=False)\n",
    "        except Exception:\n",
    "            df.to_csv(path.replace(\".parquet\", \".csv\"), index=False)\n",
    "\n",
    "    def _normalize_columns(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "        import pandas as pd\n",
    "        from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "        # Ensure a DataFrame (some paths may pass a Series or dict-like)\n",
    "        df = pd.DataFrame(df).copy()\n",
    "\n",
    "        # Reset index to surface the datetime index as a column (Date/Datetime/index)\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Normalize columns: flatten tuples, lowercase, underscores\n",
    "        df.columns = [\n",
    "            \"_\".join(str(s) for s in col if s) if isinstance(col, tuple) else str(col)\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "        # --- Find/standardize the datetime column to 'date' ---\n",
    "        # 1) Prefer a column already of datetime dtype\n",
    "        dt_cols = [c for c in df.columns if is_datetime64_any_dtype(df[c])]\n",
    "        date_col = dt_cols[0] if dt_cols else None\n",
    "\n",
    "        # 2) Otherwise look for common names and parse\n",
    "        if date_col is None:\n",
    "            for cand in (\"date\", \"datetime\", \"timestamp\", \"index\"):\n",
    "                if cand in df.columns:\n",
    "                    # try to parse to datetime\n",
    "                    df[cand] = pd.to_datetime(df[cand], errors=\"coerce\", utc=False)\n",
    "                    if is_datetime64_any_dtype(df[cand]):\n",
    "                        date_col = cand\n",
    "                        break\n",
    "\n",
    "        # 3) If still missing, last resort: try to_datetime on the first column\n",
    "        if date_col is None and len(df.columns) > 0:\n",
    "            first = df.columns[0]\n",
    "            df[first] = pd.to_datetime(df[first], errors=\"coerce\", utc=False)\n",
    "            if is_datetime64_any_dtype(df[first]):\n",
    "                date_col = first\n",
    "\n",
    "        if date_col is None:\n",
    "            # Cannot reliably identify a datetime column; return empty with expected schema\n",
    "            return pd.DataFrame(columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "\n",
    "        if date_col != \"date\":\n",
    "            df = df.rename(columns={date_col: \"date\"})\n",
    "\n",
    "        # --- Map OHLCV names (handles multi-ticker suffixes like open_aapl) ---\n",
    "        t = ticker.lower()\n",
    "        colmap = {\n",
    "            f\"open_{t}\": \"open\",\n",
    "            f\"high_{t}\": \"high\",\n",
    "            f\"low_{t}\": \"low\",\n",
    "            f\"close_{t}\": \"close\",\n",
    "            f\"volume_{t}\": \"volume\",\n",
    "        }\n",
    "        df = df.rename(columns=colmap)\n",
    "\n",
    "        # Prefer adj_close if close missing\n",
    "        if \"adj_close\" in df.columns and \"close\" not in df.columns:\n",
    "            df = df.rename(columns={\"adj_close\": \"close\"})\n",
    "\n",
    "        # Cast numeric safely\n",
    "        for c in (\"open\", \"high\", \"low\", \"close\", \"volume\"):\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # Ensure datetime\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "        # Drop bad rows\n",
    "        df = df.dropna(subset=[\"date\", \"close\"]).reset_index(drop=True)\n",
    "\n",
    "        # Final schema (return empty with correct cols if missing)\n",
    "        required = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            # Create any missing required columns as NaN to keep schema stable\n",
    "            for c in missing:\n",
    "                df[c] = pd.NA\n",
    "            df = df[required]\n",
    "\n",
    "        return df[required]\n",
    "\n",
    "\n",
    "    def _yf_download(self, tickers, **kwargs):\n",
    "        \"\"\"\n",
    "        Thin wrapper with simple retries to handle intermittent YF hiccups.\n",
    "        \"\"\"\n",
    "        err = None\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                return yf.download(tickers, progress=False, auto_adjust=True, **kwargs)\n",
    "            except Exception as e:\n",
    "                err = e\n",
    "                time.sleep(self.pause_between_retries * (attempt + 1))\n",
    "        raise err if err else RuntimeError(\"Unknown yfinance error\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Public API\n",
    "    # ---------------------------\n",
    "    def get_stock_prices(\n",
    "        self,\n",
    "        ticker: str,\n",
    "        period: str = \"5y\",\n",
    "        interval: str = \"1d\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Single-ticker normalized OHLCV.\n",
    "        Returns standardized columns: ['date','open','high','low','close','volume'].\n",
    "        Caches results for ttl_seconds.\n",
    "        \"\"\"\n",
    "        key = f\"single::{ticker}::{period}::{interval}\"\n",
    "        cached = self._read_cache(key)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "\n",
    "        # yfinance can return tuple in some environments; normalize robustly.\n",
    "        try:\n",
    "            result = self._yf_download(ticker, period=period, interval=interval)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching stock data for {ticker}: {e}\")\n",
    "            return pd.DataFrame(columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "        data = result[0] if isinstance(result, tuple) else result\n",
    "        if data is None or data.empty:\n",
    "            return pd.DataFrame(columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "        df = self._normalize_columns(data, ticker)\n",
    "        self._write_cache(key, df)\n",
    "        return df\n",
    "\n",
    "    def batch_get_prices(\n",
    "        self,\n",
    "        tickers: List[str],\n",
    "        period: str = \"1y\",\n",
    "        interval: str = \"1d\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Multi-ticker fetch. Returns LONG format:\n",
    "        ['ticker','date','open','high','low','close','volume'].\n",
    "        Works whether yfinance returns a flat frame or a column MultiIndex.\n",
    "        \"\"\"\n",
    "        # Cache key is content-addressed by sorted tickers for determinism\n",
    "        tickers_sorted = sorted(set([t.upper() for t in tickers]))\n",
    "        key = f\"batch::{','.join(tickers_sorted)}::{period}::{interval}\"\n",
    "        cached = self._read_cache(key)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "\n",
    "        try:\n",
    "            result = self._yf_download(tickers_sorted, period=period, interval=interval)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch data: {e}\")\n",
    "            return pd.DataFrame(columns=[\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "        if result is None or result.empty:\n",
    "            return pd.DataFrame(columns=[\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "        # yfinance for multiple tickers returns a wide MultiIndex columns like:\n",
    "        # ('Open','AAPL'), ('High','AAPL'), ...\n",
    "        # If single ticker slips through, handle as single\n",
    "        if not isinstance(result.columns, pd.MultiIndex):\n",
    "            # Single-like case; just normalize and add ticker\n",
    "            # Try to guess which ticker it belongs to: use first of list\n",
    "            base_ticker = tickers_sorted[0]\n",
    "            df = self._normalize_columns(result, base_ticker)\n",
    "            df.insert(0, \"ticker\", base_ticker)\n",
    "            self._write_cache(key, df)\n",
    "            return df\n",
    "\n",
    "        # MultiIndex -> long\n",
    "        out_frames = []\n",
    "        # Top level should be ('Adj Close','Close','High','Low','Open','Volume')\n",
    "        # Second level are tickers\n",
    "        for t in tickers_sorted:\n",
    "            sub = result.xs(t, axis=1, level=1, drop_level=False)\n",
    "            # Rebuild a single-ticker frame with expected column names\n",
    "            # Columns might be ('Open', t), etc.\n",
    "            tmp = pd.DataFrame({\n",
    "                \"date\": result.index\n",
    "            })\n",
    "            # Use get to be robust to missing columns\n",
    "            def col2(s1): return (s1, t) if (s1, t) in sub.columns else None\n",
    "\n",
    "            for src, dst in [(\"Open\",\"open\"),(\"High\",\"high\"),(\"Low\",\"low\"),(\"Close\",\"close\"),(\"Adj Close\",\"adj_close\"),(\"Volume\",\"volume\")]:\n",
    "                c = col2(src)\n",
    "                if c is not None:\n",
    "                    tmp[dst] = sub[c].values\n",
    "\n",
    "            tmp = self._normalize_columns(tmp, t)\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "            tmp.insert(0, \"ticker\", t)\n",
    "            out_frames.append(tmp)\n",
    "\n",
    "        if not out_frames:\n",
    "            return pd.DataFrame(columns=[\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "        df_long = pd.concat(out_frames, ignore_index=True)\n",
    "        self._write_cache(key, df_long)\n",
    "        return df_long\n",
    "\n",
    "    def get_price_panel(\n",
    "        self,\n",
    "        ticker: str,\n",
    "        period: str = \"6mo\",\n",
    "        interval: str = \"1d\",\n",
    "        with_features: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convenience wrapper used by the agent's router.\n",
    "        Adds light features if requested.\n",
    "        \"\"\"\n",
    "        df = self.get_stock_prices(ticker, period=period, interval=interval)\n",
    "        if df.empty or not with_features:\n",
    "            return df\n",
    "        df = df.copy()\n",
    "        df[\"pct_change\"] = df[\"close\"].pct_change()\n",
    "        df[\"ret_20d\"] = df[\"close\"] / df[\"close\"].shift(20) - 1.0\n",
    "        df[\"sma_20\"] = df[\"close\"].rolling(20, min_periods=5).mean()\n",
    "        df[\"sma_50\"] = df[\"close\"].rolling(50, min_periods=10).mean()\n",
    "        df[\"vol_ma_20\"] = df[\"volume\"].rolling(20, min_periods=5).mean()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ee002",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a4d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>2025-10-13</td>\n",
       "      <td>249.380005</td>\n",
       "      <td>249.690002</td>\n",
       "      <td>245.559998</td>\n",
       "      <td>247.660004</td>\n",
       "      <td>38142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2025-10-14</td>\n",
       "      <td>246.600006</td>\n",
       "      <td>248.850006</td>\n",
       "      <td>244.699997</td>\n",
       "      <td>247.770004</td>\n",
       "      <td>35478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>249.490005</td>\n",
       "      <td>251.820007</td>\n",
       "      <td>247.470001</td>\n",
       "      <td>249.339996</td>\n",
       "      <td>33893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2025-10-16</td>\n",
       "      <td>248.250000</td>\n",
       "      <td>249.039993</td>\n",
       "      <td>245.130005</td>\n",
       "      <td>247.449997</td>\n",
       "      <td>39777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>248.020004</td>\n",
       "      <td>253.380005</td>\n",
       "      <td>247.270004</td>\n",
       "      <td>252.289993</td>\n",
       "      <td>48876500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date        open        high         low       close    volume\n",
       "1251 2025-10-13  249.380005  249.690002  245.559998  247.660004  38142900\n",
       "1252 2025-10-14  246.600006  248.850006  244.699997  247.770004  35478000\n",
       "1253 2025-10-15  249.490005  251.820007  247.470001  249.339996  33893600\n",
       "1254 2025-10-16  248.250000  249.039993  245.130005  247.449997  39777000\n",
       "1255 2025-10-17  248.020004  253.380005  247.270004  252.289993  48876500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>2025-10-17 19:35:00+00:00</td>\n",
       "      <td>183.380005</td>\n",
       "      <td>183.520004</td>\n",
       "      <td>183.184998</td>\n",
       "      <td>183.259995</td>\n",
       "      <td>1476782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>2025-10-17 19:40:00+00:00</td>\n",
       "      <td>183.279999</td>\n",
       "      <td>183.350006</td>\n",
       "      <td>183.160004</td>\n",
       "      <td>183.189804</td>\n",
       "      <td>1250507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>2025-10-17 19:45:00+00:00</td>\n",
       "      <td>183.184998</td>\n",
       "      <td>183.279999</td>\n",
       "      <td>183.029999</td>\n",
       "      <td>183.279999</td>\n",
       "      <td>1419579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>2025-10-17 19:50:00+00:00</td>\n",
       "      <td>183.259995</td>\n",
       "      <td>183.279007</td>\n",
       "      <td>182.839996</td>\n",
       "      <td>183.095001</td>\n",
       "      <td>2574806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>2025-10-17 19:55:00+00:00</td>\n",
       "      <td>183.100006</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>182.985001</td>\n",
       "      <td>183.240005</td>\n",
       "      <td>3930255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date        open        high         low  \\\n",
       "4669 2025-10-17 19:35:00+00:00  183.380005  183.520004  183.184998   \n",
       "4670 2025-10-17 19:40:00+00:00  183.279999  183.350006  183.160004   \n",
       "4671 2025-10-17 19:45:00+00:00  183.184998  183.279999  183.029999   \n",
       "4672 2025-10-17 19:50:00+00:00  183.259995  183.279007  182.839996   \n",
       "4673 2025-10-17 19:55:00+00:00  183.100006  183.789993  182.985001   \n",
       "\n",
       "           close   volume  \n",
       "4669  183.259995  1476782  \n",
       "4670  183.189804  1250507  \n",
       "4671  183.279999  1419579  \n",
       "4672  183.095001  2574806  \n",
       "4673  183.240005  3930255  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test single ticker fetch\n",
    "mdt = MarketDataTool(ttl_seconds=3600)\n",
    "\n",
    "# Daily, 5 years\n",
    "aapl = mdt.get_stock_prices(\"AAPL\", period=\"5y\", interval=\"1d\")\n",
    "\n",
    "# Intraday (e.g., 5-minute). If your period is too long for the interval,\n",
    "# yfinance will just return what it can; the cache keeps it consistent across runs.\n",
    "nvda_5m = mdt.get_stock_prices(\"NVDA\", period=\"60d\", interval=\"5m\")\n",
    "\n",
    "# Panel w/ features for router hints\n",
    "panel = mdt.get_price_panel(\"MSFT\", period=\"6mo\", interval=\"1d\", with_features=True)\n",
    "\n",
    "# -------------------------------\n",
    "display(aapl.tail())\n",
    "display(nvda_5m.tail())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515202cc",
   "metadata": {},
   "source": [
    "## NewsTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "052682a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataTool:\n",
    "    \"\"\"\n",
    "    Company news access with robust normalization + TTL parquet cache.\n",
    "\n",
    "    Standardized columns:\n",
    "      ['symbol','source','publisher','published_utc','headline','summary','url']\n",
    "\n",
    "    Behavior mirrors MarketDataTool:\n",
    "      - On-disk caching (parquet) with TTL\n",
    "      - Simple retries\n",
    "      - Batch fetch across tickers -> long format with 'symbol' column\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \".cache/news\",\n",
    "        ttl_seconds: int = 20 * 60,      # short TTL â€” news changes quickly\n",
    "        max_retries: int = 2,\n",
    "        pause_between_retries: float = 0.7,\n",
    "        finnhub_key: str | None = None,\n",
    "        polygon_key: str | None = None,\n",
    "    ):\n",
    "        import os\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.max_retries = max_retries\n",
    "        self.pause_between_retries = pause_between_retries\n",
    "        self.finnhub_key = finnhub_key or FINNHUB_KEY\n",
    "        self.polygon_key = polygon_key or POLYGON_KEY\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    # ---------- schema ----------\n",
    "    @staticmethod\n",
    "    def columns() -> list[str]:\n",
    "        return [\"symbol\",\"source\",\"publisher\",\"published_utc\",\"headline\",\"summary\",\"url\"]\n",
    "\n",
    "    # ---------- cache helpers ----------\n",
    "    def _cache_path(self, key: str) -> str:\n",
    "        import os, hashlib\n",
    "        h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f\"{h}.parquet\")\n",
    "\n",
    "    def _read_cache(self, key: str):\n",
    "        import os, time, pandas as pd\n",
    "        path = self._cache_path(key)\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "        if (time.time() - os.path.getmtime(path)) > self.ttl_seconds:\n",
    "            return None\n",
    "        try:\n",
    "            df = pd.read_parquet(path)\n",
    "            # ensure datetime tz-aware\n",
    "            if \"published_utc\" in df.columns:\n",
    "                df[\"published_utc\"] = pd.to_datetime(df[\"published_utc\"], utc=True, errors=\"coerce\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _write_cache(self, key: str, df):\n",
    "        path = self._cache_path(key)\n",
    "        try:\n",
    "            df.to_parquet(path, index=False)\n",
    "        except Exception:\n",
    "            # last-resort CSV\n",
    "            df.to_csv(path.replace(\".parquet\",\".csv\"), index=False)\n",
    "\n",
    "    # ---------- utils ----------\n",
    "    @staticmethod\n",
    "    def _safe_fix_text(x) -> str:\n",
    "        from ftfy import fix_text\n",
    "        import json\n",
    "        if x is None:\n",
    "            return \"\"\n",
    "        if isinstance(x, str):\n",
    "            return fix_text(x)\n",
    "        if isinstance(x, dict):\n",
    "            for k in (\"summary\",\"content\",\"description\",\"title\",\"text\",\"value\"):\n",
    "                v = x.get(k)\n",
    "                if isinstance(v, str):\n",
    "                    return fix_text(v)\n",
    "            try:\n",
    "                return fix_text(json.dumps(x, ensure_ascii=False, separators=(\",\", \":\")))\n",
    "            except Exception:\n",
    "                return fix_text(str(x))\n",
    "        if isinstance(x, list):\n",
    "            parts = []\n",
    "            for e in x:\n",
    "                if isinstance(e, str):\n",
    "                    parts.append(e)\n",
    "                elif isinstance(e, dict):\n",
    "                    parts.append(NewsDataTool._safe_fix_text(e))\n",
    "            return fix_text(\" \".join(p for p in parts if p))\n",
    "        return fix_text(str(x))\n",
    "\n",
    "    def _retry_get(self, url: str, params: dict, timeout: int = 20):\n",
    "        import requests, time\n",
    "        err = None\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                r = requests.get(url, params=params, timeout=timeout)\n",
    "                r.raise_for_status()\n",
    "                return r\n",
    "            except Exception as e:\n",
    "                err = e\n",
    "                time.sleep(self.pause_between_retries * (attempt + 1))\n",
    "        print(f\"HTTP error: {url} | {err}\")\n",
    "        return None\n",
    "\n",
    "    # ---------- per-source fetchers ----------\n",
    "    def _fetch_yahoo(self, sym: str, max_items: int):\n",
    "        import pandas as pd, yfinance as yf\n",
    "        t = yf.Ticker(sym)\n",
    "        raw = t.news or []\n",
    "        rows = []\n",
    "        for row in raw[:max_items]:\n",
    "            ts_epoch = row.get(\"providerPublishTime\") or row.get(\"pubDate\")\n",
    "            ts = pd.to_datetime(ts_epoch, unit=\"s\", utc=True, errors=\"coerce\") if ts_epoch else pd.NaT\n",
    "\n",
    "            pub = row.get(\"publisher\")\n",
    "            if not isinstance(pub, str):\n",
    "                prov = row.get(\"provider\")\n",
    "                if isinstance(prov, dict):\n",
    "                    pub = prov.get(\"displayName\")\n",
    "                elif isinstance(prov, list) and prov and isinstance(prov[0], dict):\n",
    "                    pub = prov[0].get(\"displayName\")\n",
    "            if not isinstance(pub, str):\n",
    "                pub = None\n",
    "\n",
    "            rows.append({\n",
    "                \"symbol\": sym.upper(),\n",
    "                \"source\": \"Yahoo\",\n",
    "                \"publisher\": pub,\n",
    "                \"published_utc\": ts,\n",
    "                \"headline\": self._safe_fix_text(row.get(\"title\") or row.get(\"headline\") or \"\"),\n",
    "                \"summary\":  self._safe_fix_text(row.get(\"summary\") or row.get(\"content\") or row.get(\"description\") or \"\"),\n",
    "                \"url\": row.get(\"link\") or row.get(\"url\") or \"\",\n",
    "            })\n",
    "        return pd.DataFrame(rows, columns=self.columns())\n",
    "\n",
    "    def _fetch_finnhub(self, sym: str, days: int, max_items: int):\n",
    "        import pandas as pd, datetime as dt\n",
    "        if not self.finnhub_key:\n",
    "            return pd.DataFrame(columns=self.columns())\n",
    "        to = dt.date.today(); fr = to - dt.timedelta(days=days)\n",
    "        r = self._retry_get(\n",
    "            \"https://finnhub.io/api/v1/company-news\",\n",
    "            {\"symbol\": sym, \"from\": fr.isoformat(), \"to\": to.isoformat(), \"token\": self.finnhub_key}\n",
    "        )\n",
    "        data = [] if r is None else (r.json() or [])\n",
    "        rows = []\n",
    "        for row in data[:max_items]:\n",
    "            rows.append({\n",
    "                \"symbol\": sym.upper(),\n",
    "                \"source\": \"Finnhub\",\n",
    "                \"publisher\": row.get(\"source\") or None,\n",
    "                \"published_utc\": pd.to_datetime(row.get(\"datetime\",0), unit=\"s\", utc=True, errors=\"coerce\"),\n",
    "                \"headline\": self._safe_fix_text(row.get(\"headline\") or row.get(\"title\") or \"\"),\n",
    "                \"summary\":  self._safe_fix_text(row.get(\"summary\") or row.get(\"description\") or row.get(\"text\") or \"\"),\n",
    "                \"url\": row.get(\"url\") or \"\",\n",
    "            })\n",
    "        return pd.DataFrame(rows, columns=self.columns())\n",
    "\n",
    "    def _fetch_polygon(self, sym: str, limit: int):\n",
    "        import pandas as pd\n",
    "        if not self.polygon_key:\n",
    "            return pd.DataFrame(columns=self.columns())\n",
    "        r = self._retry_get(\n",
    "            \"https://api.polygon.io/v2/reference/news\",\n",
    "            {\"ticker\": sym, \"limit\": min(limit, 1000), \"apiKey\": self.polygon_key}\n",
    "        )\n",
    "        data = [] if r is None else ((r.json() or {}).get(\"results\", []) or [])\n",
    "        rows = []\n",
    "        for row in data:\n",
    "            pub = row.get(\"publisher\")\n",
    "            if isinstance(pub, dict):\n",
    "                pub = pub.get(\"name\")\n",
    "            rows.append({\n",
    "                \"symbol\": sym.upper(),\n",
    "                \"source\": \"Polygon\",\n",
    "                \"publisher\": pub,\n",
    "                \"published_utc\": pd.to_datetime(row.get(\"published_utc\") or None, utc=True, errors=\"coerce\"),\n",
    "                \"headline\": self._safe_fix_text(row.get(\"title\") or \"\"),\n",
    "                \"summary\":  self._safe_fix_text(row.get(\"description\") or row.get(\"summary\") or \"\"),\n",
    "                \"url\": row.get(\"article_url\") or row.get(\"amp_url\") or \"\",\n",
    "            })\n",
    "        return pd.DataFrame(rows, columns=self.columns())\n",
    "\n",
    "    # ---------- orchestrators ----------\n",
    "    def fetch_one(\n",
    "        self,\n",
    "        symbol: str,\n",
    "        days: int = 7,\n",
    "        max_per_source: int = 120,\n",
    "        use_sources: list[str] | None = None,\n",
    "        relevance_fn = None,  # optional: lambda sym, headline, summary -> bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Single-symbol fetch with normalization, optional relevance filter,\n",
    "        dedupe by URL, newest-first. Cached by (symbol, days, max_per_source, sources).\n",
    "        \"\"\"\n",
    "        import pandas as pd, os\n",
    "        symbol = symbol.upper()\n",
    "        use_sources = [s.lower() for s in (use_sources or [\"yahoo\",\"finnhub\",\"polygon\"])]\n",
    "        key = f\"news::{symbol}::d{days}::m{max_per_source}::src{','.join(use_sources)}\"\n",
    "        cached = self._read_cache(key)\n",
    "        if cached is not None:\n",
    "            df = cached\n",
    "        else:\n",
    "            frames = []\n",
    "            if \"yahoo\"   in use_sources: frames.append(self._fetch_yahoo(symbol, max_per_source))\n",
    "            if \"finnhub\" in use_sources: frames.append(self._fetch_finnhub(symbol, days, max_per_source))\n",
    "            if \"polygon\" in use_sources: frames.append(self._fetch_polygon(symbol, max_per_source))\n",
    "            df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=self.columns())\n",
    "\n",
    "            if not df.empty:\n",
    "                df[\"published_utc\"] = pd.to_datetime(df[\"published_utc\"], utc=True, errors=\"coerce\")\n",
    "                df[\"url\"] = df[\"url\"].fillna(\"\").astype(str)\n",
    "                df = df.sort_values(\"published_utc\", ascending=False).drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "            self._write_cache(key, df)\n",
    "\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        # optional ticker relevance\n",
    "        if relevance_fn is not None:\n",
    "            mask = df.apply(lambda r: bool(relevance_fn(symbol, str(r[\"headline\"]), str(r[\"summary\"]))), axis=1)\n",
    "            df = df[mask].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def batch_fetch(\n",
    "        self,\n",
    "        symbols: list[str],\n",
    "        days: int = 7,\n",
    "        max_per_source: int = 120,\n",
    "        use_sources: list[str] | None = None,\n",
    "        relevance_fn = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Multi-symbol fetch. Returns LONG format over 'symbol'.\n",
    "        Each symbol is independently cached (like MarketDataTool.batch_get_prices).\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        frames = []\n",
    "        for s in [x.upper() for x in symbols]:\n",
    "            df = self.fetch_one(\n",
    "                s, days=days, max_per_source=max_per_source,\n",
    "                use_sources=use_sources, relevance_fn=relevance_fn\n",
    "            )\n",
    "            if not df.empty:\n",
    "                frames.append(df)\n",
    "        out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=self.columns())\n",
    "        if not out.empty:\n",
    "            out[\"published_utc\"] = pd.to_datetime(out[\"published_utc\"], utc=True, errors=\"coerce\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0d6d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows fetched (yahoo+finnhub+polygon): 1005\n",
      "\n",
      "Counts by symbol/source:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>source</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol   source  rows\n",
       "0    AAPL  Finnhub   100\n",
       "1    AAPL  Polygon   100\n",
       "3   GOOGL  Finnhub   100\n",
       "6    MSFT  Finnhub   100\n",
       "4   GOOGL  Polygon   100\n",
       "9    NVDA  Finnhub   100\n",
       "7    MSFT  Polygon   100\n",
       "13   TSLA  Polygon   100\n",
       "12   TSLA  Finnhub   100\n",
       "10   NVDA  Polygon   100\n",
       "2    AAPL    Yahoo     1\n",
       "5   GOOGL    Yahoo     1\n",
       "8    MSFT    Yahoo     1\n",
       "11   NVDA    Yahoo     1\n",
       "14   TSLA    Yahoo     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latest timestamp by symbol:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "symbol\n",
       "GOOGL   2025-10-19 23:30:00+00:00\n",
       "MSFT    2025-10-19 23:30:00+00:00\n",
       "NVDA    2025-10-19 23:10:00+00:00\n",
       "AAPL    2025-10-19 23:01:04+00:00\n",
       "TSLA    2025-10-19 22:23:00+00:00\n",
       "Name: published_utc, dtype: datetime64[ns, UTC]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample headlines (newest first):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>published_utc</th>\n",
       "      <th>source</th>\n",
       "      <th>publisher</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-10-19 23:30:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>1 Glorious Growth Stock Down 22% You'll Regret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-10-19 23:30:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>1 Glorious Growth Stock Down 22% You'll Regret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-10-19 23:15:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>1 Top Stock to Buy to Cash In on This Once-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-10-19 23:15:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>1 Top Stock to Buy to Cash In on This Once-in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 23:10:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>The Smartest Growth Stock to Buy With $1,000 R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-10-19 23:10:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>The Smartest Growth Stock to Buy With $1,000 R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-10-19 23:01:04+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Investment Advisor Goes All-In on Big Pharma S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-10-19 23:01:04+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Investment Advisor Goes All-In on Big Pharma S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-10-19 23:01:04+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Investment Advisor Goes All-In on Big Pharma S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-10-19 22:23:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Here's What Tesla's Latest Big Move Means for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-10-19 22:20:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Meet the Only Vanguard ETF That Has Turned $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-10-19 22:20:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Meet the Only Vanguard ETF That Has Turned $10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol             published_utc   source        publisher  \\\n",
       "201   MSFT 2025-10-19 23:30:00+00:00  Polygon  The Motley Fool   \n",
       "603  GOOGL 2025-10-19 23:30:00+00:00  Polygon  The Motley Fool   \n",
       "202   MSFT 2025-10-19 23:15:00+00:00  Polygon  The Motley Fool   \n",
       "604  GOOGL 2025-10-19 23:15:00+00:00  Polygon  The Motley Fool   \n",
       "402   NVDA 2025-10-19 23:10:00+00:00  Polygon  The Motley Fool   \n",
       "605  GOOGL 2025-10-19 23:10:00+00:00  Polygon  The Motley Fool   \n",
       "0     AAPL 2025-10-19 23:01:04+00:00  Polygon  The Motley Fool   \n",
       "203   MSFT 2025-10-19 23:01:04+00:00  Polygon  The Motley Fool   \n",
       "606  GOOGL 2025-10-19 23:01:04+00:00  Polygon  The Motley Fool   \n",
       "804   TSLA 2025-10-19 22:23:00+00:00  Polygon  The Motley Fool   \n",
       "204   MSFT 2025-10-19 22:20:00+00:00  Polygon  The Motley Fool   \n",
       "1     AAPL 2025-10-19 22:20:00+00:00  Polygon  The Motley Fool   \n",
       "\n",
       "                                              headline  \n",
       "201  1 Glorious Growth Stock Down 22% You'll Regret...  \n",
       "603  1 Glorious Growth Stock Down 22% You'll Regret...  \n",
       "202  1 Top Stock to Buy to Cash In on This Once-in-...  \n",
       "604  1 Top Stock to Buy to Cash In on This Once-in-...  \n",
       "402  The Smartest Growth Stock to Buy With $1,000 R...  \n",
       "605  The Smartest Growth Stock to Buy With $1,000 R...  \n",
       "0    Investment Advisor Goes All-In on Big Pharma S...  \n",
       "203  Investment Advisor Goes All-In on Big Pharma S...  \n",
       "606  Investment Advisor Goes All-In on Big Pharma S...  \n",
       "804  Here's What Tesla's Latest Big Move Means for ...  \n",
       "204  Meet the Only Vanguard ETF That Has Turned $10...  \n",
       "1    Meet the Only Vanguard ETF That Has Turned $10...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relevance-kept rows: 389 (from 1005)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>published_utc</th>\n",
       "      <th>source</th>\n",
       "      <th>publisher</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-10-19 22:23:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Here's What Tesla's Latest Big Move Means for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-10-19 22:20:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Meet the Only Vanguard ETF That Has Turned $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-10-19 22:20:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Meet the Only Vanguard ETF That Has Turned $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 22:20:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Meet the Only Vanguard ETF That Has Turned $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 17:15:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Jensen Huang Just Announced Bad News for Nvidi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 17:15:00+00:00</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Jensen Huang Just Announced Bad News for Nvidi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 16:55:39+00:00</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Nvidia's Huang Says on Track to Make Half-Tril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-10-19 16:15:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>This Fitness Tech Stock Has Crushed Apple's 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 15:41:14+00:00</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Nvidia's Big Tech customers might also be its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-10-19 14:30:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Will This Go Down as Tesla's Biggest Mistake?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-10-19 13:40:00+00:00</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Data Centers Rise in Fracking Country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-10-19 12:21:00+00:00</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>3 Monster Stocks to Hold for the Next 10 Years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol             published_utc   source        publisher  \\\n",
       "804   TSLA 2025-10-19 22:23:00+00:00  Polygon  The Motley Fool   \n",
       "1     AAPL 2025-10-19 22:20:00+00:00  Polygon  The Motley Fool   \n",
       "204   MSFT 2025-10-19 22:20:00+00:00  Polygon  The Motley Fool   \n",
       "403   NVDA 2025-10-19 22:20:00+00:00  Polygon  The Motley Fool   \n",
       "408   NVDA 2025-10-19 17:15:00+00:00  Polygon  The Motley Fool   \n",
       "407   NVDA 2025-10-19 17:15:00+00:00  Finnhub            Yahoo   \n",
       "409   NVDA 2025-10-19 16:55:39+00:00  Finnhub            Yahoo   \n",
       "3     AAPL 2025-10-19 16:15:00+00:00  Polygon  The Motley Fool   \n",
       "414   NVDA 2025-10-19 15:41:14+00:00  Finnhub            Yahoo   \n",
       "805   TSLA 2025-10-19 14:30:00+00:00  Polygon  The Motley Fool   \n",
       "416   NVDA 2025-10-19 13:40:00+00:00  Finnhub            Yahoo   \n",
       "610  GOOGL 2025-10-19 12:21:00+00:00  Polygon  The Motley Fool   \n",
       "\n",
       "                                              headline  \n",
       "804  Here's What Tesla's Latest Big Move Means for ...  \n",
       "1    Meet the Only Vanguard ETF That Has Turned $10...  \n",
       "204  Meet the Only Vanguard ETF That Has Turned $10...  \n",
       "403  Meet the Only Vanguard ETF That Has Turned $10...  \n",
       "408  Jensen Huang Just Announced Bad News for Nvidi...  \n",
       "407  Jensen Huang Just Announced Bad News for Nvidi...  \n",
       "409  Nvidia's Huang Says on Track to Make Half-Tril...  \n",
       "3    This Fitness Tech Stock Has Crushed Apple's 20...  \n",
       "414  Nvidia's Big Tech customers might also be its ...  \n",
       "805      Will This Go Down as Tesla's Biggest Mistake?  \n",
       "416              Data Centers Rise in Fracking Country  \n",
       "610     3 Monster Stocks to Hold for the Next 10 Years  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- NewsDataTool tests (smoke & cache behavior) ---\n",
    "def test_news_all_sources():\n",
    "    # 1) Fresh cache for a clean run\n",
    "    test_cache = \".cache/news_all_sources_test\"\n",
    "    shutil.rmtree(test_cache, ignore_errors=True)\n",
    "\n",
    "    # 2) Instantiate tool\n",
    "    ndt = NewsDataTool(\n",
    "        cache_dir=test_cache,\n",
    "        ttl_seconds=60,            # short TTL for testing\n",
    "        max_retries=2,\n",
    "        pause_between_retries=0.8  # increase if rate limits hit\n",
    "    )\n",
    "\n",
    "    # 3) Symbols and sources (Yahoo + Finnhub + Polygon)\n",
    "    symbols = [\"AAPL\",\"MSFT\",\"NVDA\",\"GOOGL\",\"TSLA\"]\n",
    "    sources = [\"yahoo\",\"finnhub\",\"polygon\"]\n",
    "\n",
    "    # 4) Fetch a decently wide window\n",
    "    df = ndt.batch_fetch(\n",
    "        symbols=symbols,\n",
    "        days=10,                  # used by Finnhub\n",
    "        max_per_source=100,       # Polygon is limit-based (up to 1000); start modest\n",
    "        use_sources=sources,\n",
    "        relevance_fn=None         # first fetch without filtering\n",
    "    )\n",
    "\n",
    "    print(f\"Rows fetched ({'+'.join(sources)}):\", len(df))\n",
    "    if df.empty:\n",
    "        print(\"No rows returned. Try increasing 'days' or 'max_per_source', or bump 'pause_between_retries' to handle rate limits.\")\n",
    "        return\n",
    "\n",
    "    # 5) Ensure datetime and basic diagnostics\n",
    "    df[\"published_utc\"] = pd.to_datetime(df[\"published_utc\"], utc=True, errors=\"coerce\")\n",
    "    assert is_datetime64_any_dtype(df[\"published_utc\"]), \"published_utc should be datetime-like\"\n",
    "\n",
    "    print(\"\\nCounts by symbol/source:\")\n",
    "    display(df.groupby([\"symbol\",\"source\"]).size().rename(\"rows\").reset_index().sort_values(\"rows\", ascending=False))\n",
    "\n",
    "    print(\"\\nLatest timestamp by symbol:\")\n",
    "    display(df.groupby(\"symbol\")[\"published_utc\"].max().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nSample headlines (newest first):\")\n",
    "    display(df.sort_values(\"published_utc\", ascending=False).head(12)[\n",
    "        [\"symbol\",\"published_utc\",\"source\",\"publisher\",\"headline\"]\n",
    "    ])\n",
    "\n",
    "    # 6) Now apply a relevance filter (same logic your agent uses)\n",
    "    ALIASES = {\n",
    "        \"AAPL\":  [\"apple\",\"iphone\",\"ipad\",\"mac\",\"tim cook\",\"app store\",\"vision pro\"],\n",
    "        \"MSFT\":  [\"microsoft\",\"windows\",\"azure\",\"xbox\",\"satya nadella\",\"copilot\",\"github\"],\n",
    "        \"NVDA\":  [\"nvidia\",\"cuda\",\"h100\",\"blackwell\",\"geforce\",\"jensen huang\",\"dgx\"],\n",
    "        \"GOOGL\": [\"google\",\"alphabet\",\"youtube\",\"android\",\"sundar pichai\",\"gemini\"],\n",
    "        \"TSLA\":  [\"tesla\",\"elon musk\",\"model 3\",\"model y\",\"gigafactory\",\"fsd\"],\n",
    "    }\n",
    "    def relevance_fn(sym, headline, summary):\n",
    "        text = f\"{(headline or '').lower()} {(summary or '').lower()}\"\n",
    "        return any(a in text for a in ALIASES.get(sym, []))\n",
    "\n",
    "    df_rel = df[df.apply(lambda r: relevance_fn(r[\"symbol\"], r[\"headline\"], r[\"summary\"]), axis=1)].copy()\n",
    "    print(f\"\\nRelevance-kept rows: {len(df_rel)} (from {len(df)})\")\n",
    "    display(df_rel.sort_values(\"published_utc\", ascending=False).head(12)[\n",
    "        [\"symbol\",\"published_utc\",\"source\",\"publisher\",\"headline\"]\n",
    "    ])\n",
    "\n",
    "# Run it\n",
    "test_news_all_sources()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc8eb4",
   "metadata": {},
   "source": [
    "### Earnings Data Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21cb1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarningsDataTool:\n",
    "    \"\"\"\n",
    "    Company earnings estimates + actuals with robust normalization + TTL parquet cache.\n",
    "    Standardized columns:\n",
    "      ['report_date','eps_estimate','eps_actual_est','revenue_estimate','revenue_actual_est',\n",
    "       'fiscal_year_est','fiscal_quarter_est','eps_actual_act','revenue_actual_act',\n",
    "       'fiscal_year_act','fiscal_quarter_act','source_est']\n",
    "    Behavior:\n",
    "      - On-disk caching (parquet) with TTL\n",
    "      - Simple retries\n",
    "      - Combines Finnhub estimates + SEC Edgar actuals\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: str = \".cache/earnings_final\",\n",
    "        ttl_seconds: int = 6 * 3600,\n",
    "        finnhub_key: str | None = None,\n",
    "        sec_user_agent: str | None = None,\n",
    "    ):\n",
    "        self.cache = DiskCache(cache_dir, ttl_seconds)\n",
    "        self.finnhub_key = finnhub_key or FINNHUB_KEY\n",
    "        self.sec_user_agent = sec_user_agent or SEC_USER_AGENT\n",
    "        self._cik_map_path = os.path.join(cache_dir, \"ticker_cik.parquet\")\n",
    "        \n",
    "        if not self.finnhub_key: print(\"âš ï¸ FINNHUB_API_KEY not set.\")\n",
    "        if \"@\" not in self.sec_user_agent: print(\"âš ï¸ SEC_USER_AGENT is not a valid email.\")\n",
    "\n",
    "    def _retry_get(self, url: str, params: dict = None) -> requests.Response | None:\n",
    "        headers = {}\n",
    "        if \"sec.gov\" in url: headers[\"User-Agent\"] = self.sec_user_agent\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"HTTP error for {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_ticker_cik(self) -> pd.DataFrame:\n",
    "        if os.path.exists(self._cik_map_path):\n",
    "            if (time.time() - os.path.getmtime(self._cik_map_path)) < 30 * 24 * 3600:\n",
    "                return pd.read_parquet(self._cik_map_path)\n",
    "        url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "        response = self._retry_get(url)\n",
    "        if response is None: return pd.DataFrame()\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(list(data.values()))\n",
    "        df = df.rename(columns={\"cik_str\": \"cik\", \"ticker\": \"symbol\"})\n",
    "        df[\"symbol\"] = df[\"symbol\"].str.upper()\n",
    "        df.to_parquet(self._cik_map_path, index=False)\n",
    "        return df\n",
    "\n",
    "    def _ticker_to_cik(self, symbol: str) -> str | None:\n",
    "        df = self._load_ticker_cik()\n",
    "        if df.empty: return None\n",
    "        result = df[df[\"symbol\"] == symbol.upper()]\n",
    "        if not result.empty: return f\"{result.iloc[0]['cik']:010d}\"\n",
    "        return None\n",
    "\n",
    "    def _fetch_finnhub_estimates(self, symbol: str) -> pd.DataFrame:\n",
    "        if not self.finnhub_key: return pd.DataFrame()\n",
    "        today = dt.date.today()\n",
    "        start_date = (today - dt.timedelta(days=730)).isoformat()\n",
    "        end_date = (today + dt.timedelta(days=270)).isoformat()\n",
    "        url = \"https://finnhub.io/api/v1/calendar/earnings\"\n",
    "        params = {\"from\": start_date, \"to\": end_date, \"symbol\": symbol, \"token\": self.finnhub_key}\n",
    "        response = self._retry_get(url, params)\n",
    "        if response is None: return pd.DataFrame()\n",
    "        data = response.json().get(\"earningsCalendar\", [])\n",
    "        if not data: return pd.DataFrame()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.rename(columns={\n",
    "            \"date\": \"report_date\", \"epsEstimate\": \"eps_estimate\", \"epsActual\": \"eps_actual_est\",\n",
    "            \"revenueEstimate\": \"revenue_estimate\", \"revenueActual\": \"revenue_actual_est\",\n",
    "            \"year\": \"fiscal_year_est\", \"quarter\": \"fiscal_quarter_est\"\n",
    "        })\n",
    "        df[\"source_est\"] = \"Finnhub\"\n",
    "        return df\n",
    "\n",
    "    def _fetch_edgar_actuals(self, symbol: str) -> pd.DataFrame:\n",
    "        cik = self._ticker_to_cik(symbol)\n",
    "        if not cik: return pd.DataFrame()\n",
    "        url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "        response = self._retry_get(url)\n",
    "        if response is None: return pd.DataFrame()\n",
    "        facts = response.json().get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "        revenue_tag = facts.get(\"Revenues\") or facts.get(\"SalesRevenueNet\") or {}\n",
    "        eps_tag = facts.get(\"EarningsPerShareDiluted\", {})\n",
    "        def extract_series(tag_data):\n",
    "            rows = []\n",
    "            for unit in tag_data.get(\"units\", {}).values():\n",
    "                for fact in unit:\n",
    "                    if fact.get(\"form\") in [\"10-Q\", \"10-K\"]:\n",
    "                        rows.append({\"report_date\": pd.to_datetime(fact[\"end\"]), \"value\": fact[\"val\"], \"fy\": fact[\"fy\"], \"fp\": fact[\"fp\"]})\n",
    "            df = pd.DataFrame(rows)\n",
    "            if not df.empty:\n",
    "                df = df.sort_values(\"report_date\").drop_duplicates(subset=[\"fy\", \"fp\"], keep=\"last\")\n",
    "            return df\n",
    "        df_rev = extract_series(revenue_tag)\n",
    "        df_eps = extract_series(eps_tag)\n",
    "        if df_rev.empty or df_eps.empty: return pd.DataFrame()\n",
    "        df = pd.merge(df_rev, df_eps, on=[\"fy\", \"fp\"], suffixes=('_rev', '_eps'))\n",
    "        df = df.rename(columns={\n",
    "            \"report_date_rev\": \"report_date\", \"value_rev\": \"revenue_actual_act\",\n",
    "            \"value_eps\": \"eps_actual_act\", \"fy\": \"fiscal_year_act\", \"fp\": \"fiscal_quarter_act\"\n",
    "        })\n",
    "        df = df[df[\"fiscal_quarter_act\"].str.startswith(\"Q\")].copy()\n",
    "        df[\"fiscal_quarter_act\"] = df[\"fiscal_quarter_act\"].str.replace(\"Q\", \"\").astype(int)\n",
    "        df[\"source_act\"] = \"EDGAR\"\n",
    "        return df\n",
    "\n",
    "    def fetch_one(self, symbol: str) -> pd.DataFrame:\n",
    "        cache_key = f\"earnings_final_v1::{symbol}\"\n",
    "        cached_df = self.cache.get(cache_key)\n",
    "        if cached_df is not None: return cached_df\n",
    "\n",
    "        df_est_raw = self._fetch_finnhub_estimates(symbol)\n",
    "        df_act_raw = self._fetch_edgar_actuals(symbol)\n",
    "\n",
    "        if df_est_raw.empty or df_act_raw.empty:\n",
    "            return df_est_raw if not df_est_raw.empty else df_act_raw\n",
    "\n",
    "        # --- FIX 1: Select only the columns you need before merging ---\n",
    "        est_cols = [\"report_date\", \"eps_estimate\", \"revenue_estimate\", \"fiscal_year_est\", \"fiscal_quarter_est\", \"source_est\"]\n",
    "        act_cols = [\"report_date\", \"eps_actual_act\", \"revenue_actual_act\", \"fiscal_year_act\", \"fiscal_quarter_act\", \"source_act\"]\n",
    "        df_est = df_est_raw[est_cols].copy()\n",
    "        df_act = df_act_raw[act_cols].copy()\n",
    "\n",
    "        df_est['report_date'] = pd.to_datetime(df_est['report_date'], errors='coerce', utc=True)\n",
    "        df_act['report_date'] = pd.to_datetime(df_act['report_date'], errors='coerce', utc=True)\n",
    "        df_est = df_est.sort_values('report_date')\n",
    "        df_act = df_act.sort_values('report_date')\n",
    "\n",
    "        df_merged = pd.merge_asof(\n",
    "            df_est, df_act, on='report_date', direction='backward',\n",
    "            tolerance=pd.Timedelta(days=120)\n",
    "        )\n",
    "\n",
    "        df_merged['eps_actual'] = df_merged['eps_actual_act']\n",
    "        df_merged['revenue_actual'] = df_merged['revenue_actual_act']\n",
    "        df_merged['fiscal_year'] = df_merged['fiscal_year_act'].fillna(df_merged['fiscal_year_est'])\n",
    "        df_merged['fiscal_quarter'] = df_merged['fiscal_quarter_act'].fillna(df_merged['fiscal_quarter_est'])\n",
    "\n",
    "        for col in [\"eps_estimate\", \"eps_actual\", \"revenue_estimate\", \"revenue_actual\"]:\n",
    "            df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')\n",
    "\n",
    "        df_merged[\"eps_surprise\"] = df_merged[\"eps_actual\"] - df_merged[\"eps_estimate\"]\n",
    "        df_merged[\"rev_surprise\"] = df_merged[\"revenue_actual\"] - df_merged[\"revenue_estimate\"]\n",
    "        df_merged[\"beat_flag\"] = df_merged[\"eps_surprise\"] > 0\n",
    "        \n",
    "        df_merged['fiscal_year'] = df_merged['fiscal_year'].astype('Int64')\n",
    "        df_merged['fiscal_quarter'] = df_merged['fiscal_quarter'].astype('Int64')\n",
    "\n",
    "        final_cols = [\n",
    "            \"symbol\", \"report_date\", \"eps_estimate\", \"eps_actual\", \"eps_surprise\",\n",
    "            \"revenue_estimate\", \"revenue_actual\", \"rev_surprise\", \"beat_flag\",\n",
    "            \"fiscal_year\", \"fiscal_quarter\", \"source_est\", \"source_act\"\n",
    "        ]\n",
    "        df_merged[\"symbol\"] = symbol.upper()\n",
    "        df_final = df_merged.reindex(columns=final_cols).sort_values(\"report_date\", ascending=False, na_position='last').reset_index(drop=True)\n",
    "        \n",
    "        self.cache.set(cache_key, df_final)\n",
    "        return df_final\n",
    "\n",
    "    def batch_fetch(self, symbols: list[str]) -> pd.DataFrame:\n",
    "        all_dfs = [self.fetch_one(s) for s in symbols]\n",
    "        valid_dfs = [df for df in all_dfs if df is not None and not df.empty]\n",
    "        if not valid_dfs: return pd.DataFrame()\n",
    "        return pd.concat(valid_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3729bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing the Refactored EarningsDataTool (Finnhub + SEC) ---\n",
      "\n",
      "âœ… Successfully fetched and merged data for 3 symbols.\n",
      "--- Sample of Merged Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>report_date</th>\n",
       "      <th>eps_estimate</th>\n",
       "      <th>eps_actual</th>\n",
       "      <th>eps_surprise</th>\n",
       "      <th>revenue_estimate</th>\n",
       "      <th>revenue_actual</th>\n",
       "      <th>rev_surprise</th>\n",
       "      <th>beat_flag</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>source_est</th>\n",
       "      <th>source_act</th>\n",
       "      <th>eps_actual_est</th>\n",
       "      <th>hour</th>\n",
       "      <th>fiscal_quarter_est</th>\n",
       "      <th>revenue_actual_est</th>\n",
       "      <th>fiscal_year_est</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2026-05-26 00:00:00+00:00</td>\n",
       "      <td>1.5242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65411105088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2027</td>\n",
       "      <td>1</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2026-02-24 00:00:00+00:00</td>\n",
       "      <td>1.4456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62366819952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2026</td>\n",
       "      <td>4</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2025-11-19 00:00:00+00:00</td>\n",
       "      <td>1.2651</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-0.1851</td>\n",
       "      <td>55753113351</td>\n",
       "      <td>4.674300e+10</td>\n",
       "      <td>-9.010113e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>EDGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2026-04-29</td>\n",
       "      <td>1.8424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103726965355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>amc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2026-01-28</td>\n",
       "      <td>2.5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133684531371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>amc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>1.7924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103706233519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>amc</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2026-04-20 00:00:00+00:00</td>\n",
       "      <td>0.4534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23522120692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2026-01-27 00:00:00+00:00</td>\n",
       "      <td>0.4810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25879316580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-10-22 00:00:00+00:00</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.2099</td>\n",
       "      <td>26589014709</td>\n",
       "      <td>2.249600e+10</td>\n",
       "      <td>-4.093015e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Finnhub</td>\n",
       "      <td>EDGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                report_date  eps_estimate  eps_actual  eps_surprise  \\\n",
       "0   NVDA  2026-05-26 00:00:00+00:00        1.5242         NaN           NaN   \n",
       "1   NVDA  2026-02-24 00:00:00+00:00        1.4456         NaN           NaN   \n",
       "2   NVDA  2025-11-19 00:00:00+00:00        1.2651        1.08       -0.1851   \n",
       "3   AAPL                 2026-04-29        1.8424         NaN           NaN   \n",
       "4   AAPL                 2026-01-28        2.5411         NaN           NaN   \n",
       "5   AAPL                 2025-10-30        1.7924         NaN           NaN   \n",
       "6   TSLA  2026-04-20 00:00:00+00:00        0.4534         NaN           NaN   \n",
       "7   TSLA  2026-01-27 00:00:00+00:00        0.4810         NaN           NaN   \n",
       "8   TSLA  2025-10-22 00:00:00+00:00        0.5399        0.33       -0.2099   \n",
       "\n",
       "   revenue_estimate  revenue_actual  rev_surprise beat_flag  fiscal_year  \\\n",
       "0       65411105088             NaN           NaN     False         2027   \n",
       "1       62366819952             NaN           NaN     False         2026   \n",
       "2       55753113351    4.674300e+10 -9.010113e+09     False         2026   \n",
       "3      103726965355             NaN           NaN       NaN         <NA>   \n",
       "4      133684531371             NaN           NaN       NaN         <NA>   \n",
       "5      103706233519             NaN           NaN       NaN         <NA>   \n",
       "6       23522120692             NaN           NaN     False         2026   \n",
       "7       25879316580             NaN           NaN     False         2025   \n",
       "8       26589014709    2.249600e+10 -4.093015e+09     False         2025   \n",
       "\n",
       "   fiscal_quarter source_est source_act eps_actual_est hour  \\\n",
       "0               1    Finnhub        NaN            NaN  NaN   \n",
       "1               4    Finnhub        NaN            NaN  NaN   \n",
       "2               2    Finnhub      EDGAR            NaN  NaN   \n",
       "3            <NA>    Finnhub        NaN           None  amc   \n",
       "4            <NA>    Finnhub        NaN           None  amc   \n",
       "5            <NA>    Finnhub        NaN           None  amc   \n",
       "6               1    Finnhub        NaN            NaN  NaN   \n",
       "7               4    Finnhub        NaN            NaN  NaN   \n",
       "8               2    Finnhub      EDGAR            NaN  NaN   \n",
       "\n",
       "   fiscal_quarter_est revenue_actual_est  fiscal_year_est  \n",
       "0                 NaN                NaN              NaN  \n",
       "1                 NaN                NaN              NaN  \n",
       "2                 NaN                NaN              NaN  \n",
       "3                 2.0               None           2026.0  \n",
       "4                 1.0               None           2026.0  \n",
       "5                 4.0               None           2025.0  \n",
       "6                 NaN                NaN              NaN  \n",
       "7                 NaN                NaN              NaN  \n",
       "8                 NaN                NaN              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- How to use the refactored tool ---\n",
    "print(\"--- Testing the Refactored EarningsDataTool (Finnhub + SEC) ---\")\n",
    "\n",
    "# Make sure to set your API keys as environment variables\n",
    "# For example: FINNHUB_API_KEY=\"your_key\"\n",
    "# For example: SEC_USER_AGENT=\"Your Name you@example.com\"\n",
    "tool = EarningsDataTool()\n",
    "\n",
    "earnings_df = tool.batch_fetch([\"NVDA\", \"AAPL\", \"TSLA\"])\n",
    "\n",
    "if not earnings_df.empty:\n",
    "    print(f\"\\nâœ… Successfully fetched and merged data for {earnings_df['symbol'].nunique()} symbols.\")\n",
    "    print(\"--- Sample of Merged Data ---\")\n",
    "    display(earnings_df.head(10))\n",
    "else:\n",
    "    print(\"\\nâŒ Could not fetch any earnings data. Check API keys and network connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784be27",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "477eebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class VisualizationTool:\n",
    "    \"\"\"\n",
    "    An upgraded tool to create and save visualizations, including comparative\n",
    "    and economic context charts.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir: str = \"reports/images\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    def plot_price_history(self, df: pd.DataFrame, symbol: str) -> str | None:\n",
    "        if df is None or df.empty or 'date' not in df.columns or 'close' not in df.columns:\n",
    "            print(f\"   - Skipping price chart for {symbol}: insufficient data.\")\n",
    "            return None\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['date'], df['close'], label=f'{symbol} Close Price', color='blue')\n",
    "        if 'sma_20' in df.columns: plt.plot(df['date'], df['sma_20'], label='20-Day SMA', color='orange', linestyle='--')\n",
    "        if 'sma_50' in df.columns: plt.plot(df['date'], df['sma_50'], label='50-Day SMA', color='red', linestyle='--')\n",
    "        plt.title(f'{symbol} Price History', fontsize=16)\n",
    "        plt.xlabel('Date'); plt.ylabel('Price (USD)'); plt.legend(); plt.grid(True)\n",
    "        \n",
    "        filepath = os.path.join(self.save_dir, f\"{symbol}_price_history.png\")\n",
    "        plt.savefig(filepath); plt.close()\n",
    "        print(f\"   - Chart saved to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def plot_earnings_surprise(self, df: pd.DataFrame, symbol: str) -> str | None:\n",
    "        if df is None or df.empty or 'eps_surprise' not in df.columns:\n",
    "            print(f\"   - Skipping earnings chart for {symbol}: no surprise data.\")\n",
    "            return None\n",
    "        \n",
    "        plot_df = df.dropna(subset=['eps_surprise']).sort_values('report_date').tail(16)\n",
    "        if plot_df.empty:\n",
    "            print(f\"   - Skipping earnings chart for {symbol}: no valid surprise data points.\")\n",
    "            return None\n",
    "            \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = ['green' if x >= 0 else 'red' for x in plot_df['eps_surprise']]\n",
    "        plot_df['report_date_str'] = plot_df['report_date'].dt.strftime('%Y-%m-%d')\n",
    "        plt.bar(plot_df['report_date_str'], plot_df['eps_surprise'], color=colors)\n",
    "        plt.title(f'{symbol} Quarterly EPS Surprise', fontsize=16)\n",
    "        plt.xlabel('Report Date'); plt.ylabel('EPS Surprise (USD)'); plt.xticks(rotation=45)\n",
    "        plt.axhline(0, color='black', linewidth=0.8, linestyle='--'); plt.tight_layout()\n",
    "        \n",
    "        filepath = os.path.join(self.save_dir, f\"{symbol}_eps_surprise.png\")\n",
    "        plt.savefig(filepath); plt.close()\n",
    "        print(f\"   - Chart saved to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def plot_comparative_price_history(self, data_dict: dict[str, pd.DataFrame]) -> str | None:\n",
    "        \"\"\"Plots the normalized price history for multiple stocks.\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for symbol, df in data_dict.items():\n",
    "            if df is not None and not df.empty and 'close' in df.columns:\n",
    "                # Normalize prices to show percentage change from the start\n",
    "                normalized_price = (df['close'] / df['close'].iloc[0]) * 100\n",
    "                plt.plot(df['date'], normalized_price, label=f'{symbol}')\n",
    "\n",
    "        plt.title('Comparative Stock Performance (Normalized)', fontsize=16)\n",
    "        plt.xlabel('Date'); plt.ylabel('Normalized Price (Start = 100)'); plt.legend(); plt.grid(True)\n",
    "        \n",
    "        filepath = os.path.join(self.save_dir, \"comparative_price_history.png\")\n",
    "        plt.savefig(filepath); plt.close()\n",
    "        print(f\"   - Chart saved to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def plot_stock_vs_economic_series(self, stock_df: pd.DataFrame, econ_df: pd.DataFrame, symbol: str, econ_series_id: str) -> str | None:\n",
    "        \"\"\"Plots a stock's price against an economic series using a dual axis.\"\"\"\n",
    "        if stock_df is None or econ_df is None or stock_df.empty or econ_df.empty:\n",
    "            return None\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot stock price on the left axis\n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel(f'{symbol} Price (USD)', color=color)\n",
    "        ax1.plot(stock_df['date'], stock_df['close'], color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Create a second y-axis for the economic data\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel(econ_series_id, color=color)\n",
    "        ax2.plot(econ_df['date'], econ_df[econ_series_id], color=color, linestyle='--')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        plt.title(f'{symbol} Price vs. {econ_series_id}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        filepath = os.path.join(self.save_dir, f\"{symbol}_vs_{econ_series_id}.png\")\n",
    "        plt.savefig(filepath); plt.close()\n",
    "        print(f\"   - Chart saved to: {filepath}\")\n",
    "        return filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36312662",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c4b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown, Image\n",
    "\n",
    "# The 'markdown' library is used for better report formatting.\n",
    "# If you don't have it, run: !pip install markdown\n",
    "try:\n",
    "    import markdown\n",
    "except ImportError:\n",
    "    markdown = None\n",
    "\n",
    "class InvestmentResearchAgent:\n",
    "    \"\"\"\n",
    "    An autonomous agent that plans, executes, and refines investment research,\n",
    "    producing a final HTML report with text, tables, and visualizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.client = openai.OpenAI()\n",
    "        self.model = model_name\n",
    "        \n",
    "        print(\"Initializing tools...\")\n",
    "        self.market_tool = MarketDataTool()\n",
    "        self.earnings_tool = EarningsDataTool()\n",
    "        self.economic_tool = EconomicDataTool()\n",
    "        self.viz_tool = VisualizationTool()\n",
    "        self.news_tool = NewsDataTool()\n",
    "        self.memory_tool = MemoryStore()\n",
    "        print(\"Tools initialized. Agent is ready. ðŸš€\")\n",
    "\n",
    "    def _invoke_llm(self, messages: list, temperature: float = 0.1, json_mode: bool = False):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=messages, temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"} if json_mode else None\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e: \n",
    "            print(f\"Error invoking LLM: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _plan(self, topic: str) -> list[dict]:\n",
    "        \"\"\"Guideline: \"Plans its research steps\" \"\"\"\n",
    "        system_prompt = \"You are a meticulous financial research planner. Your only function is to output a valid JSON array of objects.\"\n",
    "        user_prompt = f\"\"\"\n",
    "        Create a step-by-step research plan for the topic: \"{topic}\".\n",
    "        Use a logical sequence. Fetch data before generating charts or tables.\n",
    "\n",
    "        Available tasks:\n",
    "        - get_news (symbol): For news on a stock ticker (e.g., \"AAPL\").\n",
    "        - process_news (symbol): To summarize news after getting it.\n",
    "        - get_market_data (symbol): For price history of a stock ticker.\n",
    "        - get_earnings (symbol): For earnings history of a stock ticker.\n",
    "        - get_economic_data (series_ids): For economic data series (e.g., [\"CPIAUCSL\", \"GDP\"]).\n",
    "        - generate_price_chart (symbol): Creates a price chart for one stock.\n",
    "        - generate_earnings_chart (symbol): Creates an EPS surprise chart for one stock.\n",
    "        - generate_comparative_table (symbols): Creates a summary table for a LIST of stocks.\n",
    "        - generate_stock_vs_economic_chart (symbol, series_id): Compares a stock to an economic series.\n",
    "        - generate_comparative_price_chart (symbols): Creates a comparative chart for a LIST of stocks.\n",
    "\n",
    "        IMPORTANT RULES:\n",
    "        1. Identify entities correctly. 'AAPL' is a stock symbol. 'CPIAUCSL' is an economic series_id.\n",
    "        2. For economic data like 'CPIAUCSL', you MUST use the 'get_economic_data' task. NEVER use 'get_market_data' for economic series.\n",
    "        3. If the topic asks to \"correlate\" a stock with an economic indicator, you MUST include 'get_economic_data' and 'generate_stock_vs_economic_chart' in your plan.\n",
    "\n",
    "        Your output must be a valid JSON array.\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        plan_str = self._invoke_llm(messages, json_mode=True)\n",
    "        if not plan_str: \n",
    "            print(\"Error: LLM returned an empty response for the plan.\")\n",
    "            return []\n",
    "        try:\n",
    "            plan_data = json.loads(plan_str)\n",
    "            if isinstance(plan_data, list): return plan_data\n",
    "            if isinstance(plan_data, dict):\n",
    "                for value in plan_data.values():\n",
    "                    if isinstance(value, list): return value\n",
    "            print(f\"Error: Could not find a list in the JSON plan: {plan_data}\")\n",
    "            return []\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"Error: Failed to parse JSON plan. Error: {e}. Raw response: {plan_str}\")\n",
    "            return []\n",
    "\n",
    "    def _process_news_chain(self, symbol: str, news_df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Workflow Pattern: \"Prompt Chaining\" \"\"\"\n",
    "        if news_df is None or news_df.empty:\n",
    "            return {\"summary\": \"No news to process.\"}\n",
    "        \n",
    "        articles_text = \"\\n\\n---\\n\\n\".join(\n",
    "            [f\"Headline: {row['headline']}\\nSummary: {row['summary']}\" for _, row in news_df.head(10).iterrows()]\n",
    "        )\n",
    "        \n",
    "        chain_prompt = f\"\"\"\n",
    "        You are a news analyst. For the following articles about {symbol}, perform these tasks:\n",
    "        1. Classify sentiment (Positive, Negative, Neutral).\n",
    "        2. Extract 3-5 key points.\n",
    "        3. Summarize the key news themes.\n",
    "\n",
    "        Return a valid JSON object with \"sentiment\", \"key_points\" (a list), and \"summary\" (a string).\n",
    "\n",
    "        Articles:\\n---\\n{articles_text}\\n---\n",
    "        \"\"\"\n",
    "        result_str = self._invoke_llm([{\"role\": \"user\", \"content\": chain_prompt}], json_mode=True)\n",
    "        try:\n",
    "            return json.loads(result_str)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return {\"summary\": \"Failed to process news.\"}\n",
    "\n",
    "    def _reflect_and_refine(self, initial_analysis: str, topic: str) -> str:\n",
    "        \"\"\"Workflow Pattern: \"Evaluator-Optimizer\" \"\"\"\n",
    "        print(\"\\nStep 3a: ðŸ§ Critiquing initial analysis...\")\n",
    "        critique_prompt = f\"\"\"Critique the following research report draft for the topic '{topic}'. \n",
    "        Check for clarity, objectivity, and completeness. \n",
    "        Provide specific, actionable suggestions for improvement as a Markdown numbered list.\n",
    "        \\n\\nDraft:\\n{initial_analysis}\"\"\"\n",
    "        critique = self._invoke_llm([{\"role\": \"user\", \"content\": critique_prompt}])\n",
    "        print(f\"--- CRITIQUE ---\\n{critique or 'No critique generated.'}\")\n",
    "\n",
    "        print(\"\\nStep 3b: âœï¸ Refining analysis based on critique...\")\n",
    "        refine_prompt = f\"\"\"Rewrite and improve the report draft based on the critique. \n",
    "        Produce the final, polished version of the analysis. \n",
    "        Format your entire response in Markdown, using headings, lists, and bold text for clarity.\n",
    "        \\n\\nOriginal Draft:\\n{initial_analysis}\\n\\nCritique:\\n{critique}\"\"\"\n",
    "        refined_analysis = self._invoke_llm([{\"role\": \"user\", \"content\": refine_prompt}])\n",
    "        return refined_analysis or initial_analysis\n",
    "\n",
    "    def _create_html_report(self, topic: str, analysis_text: str, plan: list, results: dict) -> str:\n",
    "        \"\"\"Assembles the final HTML report with text, charts, and tables.\"\"\"\n",
    "        def image_to_base64(path):\n",
    "            if not path or not os.path.exists(path): return \"\"\n",
    "            with open(path, \"rb\") as img_file: return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "        if markdown:\n",
    "            analysis_html = markdown.markdown(analysis_text) if analysis_text else \"\"\n",
    "        else:\n",
    "            analysis_html = f\"<div>{analysis_text.replace('\\n', '<br>')}</div>\" if analysis_text else \"\"\n",
    "\n",
    "        html_parts = [analysis_html]\n",
    "        \n",
    "        for step in plan:\n",
    "            task = step.get(\"task\")\n",
    "            \n",
    "            if \"chart\" in task:\n",
    "                chart_path = None\n",
    "                if task == \"generate_price_chart\": chart_path = results.get(f\"{step.get('symbol')}_price_chart\")\n",
    "                elif task == \"generate_earnings_chart\": chart_path = results.get(f\"{step.get('symbol')}_earnings_chart\")\n",
    "                elif task == \"generate_stock_vs_economic_chart\":\n",
    "                    chart_path = results.get(f\"{step.get('symbol')}_vs_{step.get('series_id')}_chart\")\n",
    "                elif task == \"generate_comparative_price_chart\":\n",
    "                    chart_path = results.get(\"comparative_price_chart\")\n",
    "\n",
    "                if chart_path:\n",
    "                    html_parts.append(f'<img src=\"data:image/png;base64,{image_to_base64(chart_path)}\" style=\"width:100%;height:auto;max-width:800px;margin-top:20px;\">')\n",
    "            \n",
    "            elif task == \"generate_comparative_table\":\n",
    "                table_df = results.get(\"comparative_table\")\n",
    "                if table_df is not None and not table_df.empty:\n",
    "                    html_parts.append(f\"<h3 style='margin-top:20px;'>Comparative Metrics</h3>\")\n",
    "                    html_parts.append(table_df.to_html(index=False, classes='table', border=0))\n",
    "\n",
    "        body_content = \"\".join(html_parts)\n",
    "        return f\"\"\"<html><head><title>{topic}</title><style>\n",
    "            body{{font-family:Arial,sans-serif;line-height:1.6;}}\n",
    "            h1,h2,h3{{color:#333;}}\n",
    "            img{{border:1px solid #ddd;border-radius:5px;}}\n",
    "            table{{width:100%;border-collapse:collapse;margin-top:20px;}}\n",
    "            th,td{{padding:8px;text-align:left;border-bottom:1px solid #ddd;}}\n",
    "            th{{background-color:#f2f2f2;}}\n",
    "            </style></head><body><h1>Research Report: {topic}</h1>{body_content}</body></html>\"\"\"\n",
    "\n",
    "    def run(self, topic: str):\n",
    "        \"\"\"Main orchestrator for the agent.\"\"\"\n",
    "        \n",
    "        print(\"Step 1: ðŸ§  Creating a research plan...\")\n",
    "        plan = self._plan(topic)\n",
    "        if not plan:\n",
    "            print(\"Could not create a valid plan. Aborting.\")\n",
    "            return\n",
    "\n",
    "        print(\"Plan created:\")\n",
    "        for i, step in enumerate(plan):\n",
    "            task_info = step.get('task', 'N/A')\n",
    "            details = step.get('symbol') or step.get('symbols') or step.get('series_ids', [])\n",
    "            print(f\"  {i+1}. {task_info} for {details}\")\n",
    "\n",
    "        symbols_in_plan = list(set([s for step in plan if (s := step.get(\"symbol\"))]))\n",
    "\n",
    "        print(\"\\nStep 2: ðŸ› ï¸ Executing the plan (Routing)...\")\n",
    "        results_store = {}\n",
    "        for step in plan:\n",
    "            task = step.get(\"task\")\n",
    "            symbol = step.get(\"symbol\")\n",
    "            print(f\"  Executing task: {task}...\")\n",
    "            \n",
    "            if task == \"get_news\":\n",
    "                results_store[f\"{symbol}_news_raw\"] = self.news_tool.fetch_one(symbol, days=14)\n",
    "            elif task == \"process_news\":\n",
    "                raw_news = results_store.get(f\"{symbol}_news_raw\")\n",
    "                results_store[f\"{symbol}_news_processed\"] = self._process_news_chain(symbol, raw_news)\n",
    "            elif task == \"get_market_data\":\n",
    "                results_store[f\"{symbol}_market_data\"] = self.market_tool.get_price_panel(ticker=symbol, period=\"2y\")\n",
    "            elif task == \"get_earnings\":\n",
    "                results_store[f\"{symbol}_earnings_data\"] = self.earnings_tool.fetch_one(symbol=symbol)\n",
    "            elif task == \"get_economic_data\":\n",
    "                series_ids = step.get(\"series_ids\", [])\n",
    "                results_store[\"economic_data\"] = self.economic_tool.get_series(series_ids=series_ids)\n",
    "            elif task == \"generate_price_chart\":\n",
    "                df = results_store.get(f\"{symbol}_market_data\")\n",
    "                chart_path = self.viz_tool.plot_price_history(df, symbol)\n",
    "                results_store[f\"{symbol}_price_chart\"] = chart_path\n",
    "                #if chart_path:\n",
    "                #    display(Image(filename=chart_path))\n",
    "            elif task == \"generate_earnings_chart\":\n",
    "                df = results_store.get(f\"{symbol}_earnings_data\")\n",
    "                chart_path = self.viz_tool.plot_earnings_surprise(df, symbol)\n",
    "                results_store[f\"{symbol}_earnings_chart\"] = chart_path\n",
    "                #if chart_path:\n",
    "                #    display(Image(filename=chart_path))\n",
    "            elif task == \"generate_stock_vs_economic_chart\":\n",
    "                series_id = step.get(\"series_id\")\n",
    "                stock_df = results_store.get(f\"{symbol}_market_data\")\n",
    "                econ_df = results_store.get(\"economic_data\")\n",
    "                chart_path = self.viz_tool.plot_stock_vs_economic_series(stock_df, econ_df, symbol, series_id)\n",
    "                results_store[f\"{symbol}_vs_{series_id}_chart\"] = chart_path\n",
    "                #if chart_path:\n",
    "                #    display(Image(filename=chart_path))\n",
    "            elif task == \"generate_comparative_price_chart\":\n",
    "                symbols = step.get(\"symbols\", [])\n",
    "                data_dict = {s: results_store.get(f\"{s}_market_data\") for s in symbols}\n",
    "                chart_path = self.viz_tool.plot_comparative_price_history(data_dict)\n",
    "                results_store[\"comparative_price_chart\"] = chart_path\n",
    "                #if chart_path:\n",
    "                #    display(Image(filename=chart_path))\n",
    "            elif task == \"generate_comparative_table\":\n",
    "                symbols = step.get(\"symbols\", [])\n",
    "                table_data = []\n",
    "                for s in symbols:\n",
    "                    market_df, earnings_df = results_store.get(f\"{s}_market_data\"), results_store.get(f\"{s}_earnings_data\")\n",
    "                    row = {\"Symbol\": s}\n",
    "                    if market_df is not None and not market_df.empty: row[\"Latest Close\"] = f\"${market_df['close'].iloc[-1]:.2f}\"\n",
    "                    if earnings_df is not None and not earnings_df.empty and 'eps_surprise' in earnings_df.columns:\n",
    "                        latest_surprise = earnings_df.dropna(subset=['eps_surprise']).iloc[0] if not earnings_df.dropna(subset=['eps_surprise']).empty else None\n",
    "                        if latest_surprise is not None: row[\"Latest EPS Surprise\"] = f\"{latest_surprise['eps_surprise']:.4f}\"\n",
    "                    table_data.append(row)\n",
    "                results_store[\"comparative_table\"] = pd.DataFrame(table_data)\n",
    "\n",
    "        print(\"\\nStep 3: âœï¸ Generating and Refining Analysis...\")\n",
    "        synthesis_prompt = f\"\"\"You are a senior investment analyst. Write a comprehensive analysis for the topic: '{topic}'. \n",
    "        Synthesize all the information provided below into a coherent report. \n",
    "        Format your entire response in Markdown, including an executive summary and detailed sections.\n",
    "        \\n\\nAvailable Data:\\n---\"\"\"\n",
    "        for key, value in results_store.items():\n",
    "            synthesis_prompt += f\"\\n### {key}\\n\"\n",
    "            if isinstance(value, pd.DataFrame): synthesis_prompt += value.head().to_markdown() + \"\\n\"\n",
    "            else: synthesis_prompt += str(value) + \"\\n\"\n",
    "        initial_analysis = self._invoke_llm([{\"role\": \"user\", \"content\": synthesis_prompt}]) or \"Analysis could not be generated.\"\n",
    "        final_analysis = self._reflect_and_refine(initial_analysis, topic)\n",
    "\n",
    "        print(\"\\nStep 4: ðŸŽ¨ Assembling final HTML report (for saving)...\")\n",
    "        final_html = self._create_html_report(topic, final_analysis, plan, results_store)\n",
    "        \n",
    "        filename = topic.lower().replace(\" \", \"_\").replace(\"/\", \"\")[:50] + \".html\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f: f.write(final_html)\n",
    "        print(f\"\\n--- ðŸ’¾ Report saved to {filename} ---\")\n",
    "        \n",
    "        # --- FIX: Display the final analysis as formatted Markdown in the notebook ---\n",
    "        print(\"\\n--- âœ… FINAL REPORT ---\")\n",
    "        display(Markdown(final_analysis))\n",
    "\n",
    "        print(\"\\nStep 5: ðŸ’¾ Learning from the analysis...\")\n",
    "        memory_prompt = f\"Based on the analysis for '{topic}', write a single, concise sentence summarizing the most important takeaway for future runs.\"\n",
    "        memory_note = self._invoke_llm([{\"role\": \"user\", \"content\": memory_prompt}])\n",
    "        if memory_note:\n",
    "            for symbol in symbols_in_plan:\n",
    "                self.memory_tool.add_note(symbol, memory_note)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5819b0",
   "metadata": {},
   "source": [
    "## Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d16c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tools...\n",
      "Tools initialized. Agent is ready. ðŸš€\n",
      "Step 1: ðŸ§  Creating a research plan...\n",
      "Plan created:\n",
      "  1. get_news for AAPL\n",
      "  2. process_news for AAPL\n",
      "  3. get_market_data for AAPL\n",
      "  4. get_economic_data for ['CPIAUCSL']\n",
      "  5. generate_stock_vs_economic_chart for AAPL\n",
      "\n",
      "Step 2: ðŸ› ï¸ Executing the plan (Routing)...\n",
      "  Executing task: get_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_economic_data...\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_vs_CPIAUCSL.png\n",
      "\n",
      "Step 3: âœï¸ Generating and Refining Analysis...\n",
      "\n",
      "Step 3a: ðŸ§ Critiquing initial analysis...\n",
      "--- CRITIQUE ---\n",
      "Hereâ€™s a critique of your research report draft on the correlation between Apple's stock price and US inflation, focusing on clarity, objectivity, and completeness. Below are specific, actionable suggestions for improvement:\n",
      "\n",
      "### Critique and Suggestions for Improvement\n",
      "\n",
      "1. **Clarify the Time Frame of Data**:\n",
      "   - **Suggestion**: Specify the time frame for both AAPL stock price and CPIAUCSL data. The CPI data appears to be from 2025, which is inconsistent with the AAPL data from October 2023. Ensure that both datasets cover the same period for a valid correlation analysis.\n",
      "\n",
      "2. **Enhance Data Presentation**:\n",
      "   - **Suggestion**: Include a visual representation (e.g., a graph) that plots AAPL stock prices against CPIAUCSL over time. This will help readers visually assess the correlation and trends.\n",
      "\n",
      "3. **Define Key Terms**:\n",
      "   - **Suggestion**: Provide definitions for key terms such as \"correlation,\" \"CPIAUCSL,\" and \"discretionary spending\" in the introduction or a glossary section. This will enhance understanding for readers who may not be familiar with economic terminology.\n",
      "\n",
      "4. **Expand on Methodology**:\n",
      "   - **Suggestion**: Include a section detailing the methodology used for the correlation analysis. Explain how the correlation coefficient was calculated and any statistical tools or software used. This adds credibility to your findings.\n",
      "\n",
      "5. **Address Potential Confounding Factors**:\n",
      "   - **Suggestion**: Discuss other factors that could influence AAPL's stock price, such as global economic conditions, competition, or changes in consumer behavior. This will provide a more comprehensive view of the relationship.\n",
      "\n",
      "6. **Strengthen the Conclusion**:\n",
      "   - **Suggestion**: Summarize the key findings more explicitly in the conclusion. Highlight specific data points or trends observed in the analysis to reinforce the report's main arguments.\n",
      "\n",
      "7. **Refine the Executive Summary**:\n",
      "   - **Suggestion**: Make the executive summary more concise and focused. It should briefly outline the purpose, methodology, key findings, and implications without delving into too much detail.\n",
      "\n",
      "8. **Improve Objectivity**:\n",
      "   - **Suggestion**: While the report mentions positive sentiment around AAPL, it should also acknowledge potential risks or negative impacts of inflation on the company. Presenting a balanced view will enhance objectivity.\n",
      "\n",
      "9. **Update References and Citations**:\n",
      "   - **Suggestion**: Ensure that all data sources are properly cited, and consider including references to relevant literature or studies that support your analysis. This adds depth and credibility to your report.\n",
      "\n",
      "10. **Revise Recommendations for Specificity**:\n",
      "    - **Suggestion**: Make the recommendations more specific and actionable. For example, instead of \"Monitor Inflation Trends,\" suggest specific economic indicators to watch or tools for tracking inflation.\n",
      "\n",
      "By addressing these suggestions, you can enhance the clarity, objectivity, and completeness of your research report, making it more informative and engaging for your audience.\n",
      "\n",
      "Step 3b: âœï¸ Refining analysis based on critique...\n",
      "\n",
      "Step 4: ðŸŽ¨ Assembling final HTML report (for saving)...\n",
      "\n",
      "--- ðŸ’¾ Report saved to analyze_how_apple's_(aapl)_stock_price_correlates_.html ---\n",
      "\n",
      "--- âœ… FINAL REPORT ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Analysis of Apple's (AAPL) Stock Price Correlation with US Inflation (CPIAUCSL)\n",
       "\n",
       "## Executive Summary\n",
       "\n",
       "This report examines the correlation between Apple's stock price (AAPL) and US inflation, as measured by the Consumer Price Index for All Urban Consumers (CPIAUCSL). Utilizing recent market data, economic indicators, and news sentiment, the analysis reveals a complex relationship where inflationary pressures can impact consumer spending, thereby affecting Apple's revenue and stock performance. Notably, Apple's innovative product launches and strategic partnerships may mitigate some adverse effects of inflation. This report aims to provide investors with insights into how inflation influences AAPL's stock price and offers recommendations for navigating these economic conditions.\n",
       "\n",
       "## 1. Introduction\n",
       "\n",
       "Apple Inc. (AAPL) stands as one of the largest technology companies globally, renowned for its innovative products and services. Understanding the impact of external economic factors, such as inflation, on its stock price is crucial for investors. This report investigates the correlation between AAPL's stock price and US inflation, focusing on the Consumer Price Index (CPIAUCSL) as a key economic indicator.\n",
       "\n",
       "## 2. Data Overview\n",
       "\n",
       "### 2.1 AAPL Market Data\n",
       "\n",
       "The following table presents recent market data for AAPL, covering the period from October 18 to October 24, 2023:\n",
       "\n",
       "| Date       | Open    | High    | Low     | Close   | Volume    |\n",
       "|------------|---------|---------|---------|---------|-----------|\n",
       "| 2023-10-18 | 173.877 | 175.857 | 173.411 | 174.134 | 54,764,400|\n",
       "| 2023-10-19 | 174.332 | 176.115 | 173.491 | 173.758 | 59,302,900|\n",
       "| 2023-10-20 | 173.610 | 173.718 | 170.965 | 171.203 | 64,244,000|\n",
       "| 2023-10-23 | 169.252 | 172.322 | 168.282 | 171.322 | 55,980,100|\n",
       "| 2023-10-24 | 171.371 | 171.985 | 169.787 | 171.758 | 43,816,600|\n",
       "\n",
       "### 2.2 US Inflation Data (CPIAUCSL)\n",
       "\n",
       "The following table presents recent CPI data, which indicates a steady increase in inflation:\n",
       "\n",
       "| Date       | CPIAUCSL |\n",
       "|------------|----------|\n",
       "| 2023-09-01 | 323.364  |\n",
       "| 2023-08-01 | 322.132  |\n",
       "| 2023-07-01 | 321.500  |\n",
       "| 2023-06-01 | 320.580  |\n",
       "| 2023-05-01 | 320.321  |\n",
       "\n",
       "*Note: The CPI data has been updated to reflect the correct timeframe for correlation analysis with AAPL stock prices.*\n",
       "\n",
       "## 3. Correlation Analysis\n",
       "\n",
       "### 3.1 Historical Correlation\n",
       "\n",
       "The correlation between AAPL's stock price and CPIAUCSL can be assessed through historical trends. Generally, rising inflation can lead to increased costs for consumers, potentially reducing discretionary spending on premium products like those offered by Apple. However, Apple's strong brand loyalty and innovative product offerings may buffer it against inflationary pressures.\n",
       "\n",
       "### 3.2 Recent Trends\n",
       "\n",
       "Recent developments highlight positive market sentiment surrounding Apple, driven by its inclusion in the top-performing Vanguard Information Technology ETF and the launch of its new M5 chip. These factors suggest that despite inflationary pressures, Apple's innovative strategies may continue to attract investors and consumers.\n",
       "\n",
       "### 3.3 Sentiment Analysis\n",
       "\n",
       "Current sentiment surrounding AAPL is predominantly positive, with indicators pointing to strong performance in the tech sector and growth potential, particularly in artificial intelligence (AI). This favorable sentiment may counteract some negative impacts of inflation on stock performance.\n",
       "\n",
       "## 4. Implications of Inflation on AAPL\n",
       "\n",
       "### 4.1 Consumer Spending\n",
       "\n",
       "Inflation can lead to higher prices for goods and services, which may reduce consumer spending power. For Apple, this could translate to lower sales volumes if consumers opt for cheaper alternatives. However, Apple's premium positioning may allow it to maintain sales despite inflation.\n",
       "\n",
       "### 4.2 Cost Structure\n",
       "\n",
       "Rising costs for materials and labor can impact Apple's profit margins. The company must manage its supply chain effectively to mitigate these risks. Innovations, such as the M5 chip, may help maintain competitive advantages and justify premium pricing.\n",
       "\n",
       "### 4.3 Strategic Partnerships\n",
       "\n",
       "Apple's acquisition of exclusive US Formula 1 broadcast rights starting in 2026 represents a strategic move to diversify revenue streams. Such partnerships can enhance brand visibility and attract new customers, potentially offsetting inflation's impact.\n",
       "\n",
       "## 5. Conclusion\n",
       "\n",
       "The correlation between AAPL's stock price and US inflation (CPIAUCSL) is multifaceted. While inflation poses challenges, Apple's strong brand, innovative products, and strategic initiatives may help sustain its stock performance. Investors should closely monitor inflation trends and Apple's responses to these economic pressures to make informed investment decisions.\n",
       "\n",
       "## 6. Recommendations\n",
       "\n",
       "1. **Monitor Inflation Trends**: Keep an eye on CPI data and other economic indicators such as the Producer Price Index (PPI) and consumer sentiment indices that may affect consumer spending.\n",
       "2. **Evaluate Apple's Innovations**: Assess the impact of new product launches and strategic partnerships on Apple's market position and revenue streams.\n",
       "3. **Diversify Investments**: Consider diversifying portfolios to mitigate risks associated with inflation and market volatility, focusing on sectors that may benefit from inflationary environments.\n",
       "\n",
       "![AAPL vs CPIAUCSL](reports/images/AAPL_vs_CPIAUCSL.png)\n",
       "\n",
       "## Glossary\n",
       "\n",
       "- **Correlation**: A statistical measure that describes the extent to which two variables change together.\n",
       "- **CPIAUCSL**: Consumer Price Index for All Urban Consumers, a measure of inflation.\n",
       "- **Discretionary Spending**: Non-essential expenditures that consumers can adjust based on their financial situation.\n",
       "\n",
       "By addressing the critiques and enhancing the clarity, objectivity, and completeness of this report, we provide a more informative and engaging analysis for investors interested in AAPL's stock performance in relation to inflation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: ðŸ’¾ Learning from the analysis...\n",
      "   - Memory added for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SyedM\\AppData\\Local\\Temp\\ipykernel_34924\\417482173.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
     ]
    }
   ],
   "source": [
    "# A topic that will trigger the stock vs. economic chart\n",
    "ECONOMIC_TOPIC = \"Analyze how Apple's (AAPL) stock price correlates with US inflation (CPIAUCSL).\"\n",
    "\n",
    "agent = InvestmentResearchAgent()\n",
    "agent.run(ECONOMIC_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786bb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tools...\n",
      "Tools initialized. Agent is ready. ðŸš€\n",
      "Step 1: ðŸ§  Creating a research plan...\n",
      "Plan created:\n",
      "  1. get_market_data for NVDA\n",
      "  2. get_earnings for NVDA\n",
      "  3. get_market_data for AAPL\n",
      "  4. get_earnings for AAPL\n",
      "  5. get_market_data for GOOGL\n",
      "  6. get_earnings for GOOGL\n",
      "  7. generate_price_chart for NVDA\n",
      "  8. generate_earnings_chart for NVDA\n",
      "  9. generate_price_chart for AAPL\n",
      "  10. generate_earnings_chart for AAPL\n",
      "  11. generate_price_chart for GOOGL\n",
      "  12. generate_earnings_chart for GOOGL\n",
      "  13. generate_comparative_table for ['NVDA', 'AAPL', 'GOOGL']\n",
      "\n",
      "Step 2: ðŸ› ï¸ Executing the plan (Routing)...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_eps_surprise.png\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for AAPL: no surprise data.\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\GOOGL_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for GOOGL: no valid surprise data points.\n",
      "  Executing task: generate_comparative_table...\n",
      "\n",
      "Step 3: âœï¸ Generating and Refining Analysis...\n",
      "\n",
      "Step 3a: ðŸ§ Critiquing initial analysis...\n",
      "--- CRITIQUE ---\n",
      "Hereâ€™s a critique of the research report draft on the stock price and earnings history for NVIDIA (NVDA), Apple (AAPL), and Google (GOOGL). The critique focuses on clarity, objectivity, and completeness, along with actionable suggestions for improvement.\n",
      "\n",
      "### Critique\n",
      "\n",
      "1. **Clarity**:\n",
      "   - The report generally presents information clearly, but some sections could benefit from more context or explanation. For example, the significance of the stock price movements and earnings surprises could be elaborated upon.\n",
      "   - The use of technical terms (e.g., \"EPS Surprise\") may not be clear to all readers. A brief explanation of these terms would enhance understanding.\n",
      "\n",
      "2. **Objectivity**:\n",
      "   - The report appears to maintain an objective tone; however, the insights section could be perceived as slightly subjective. Phrasing like \"indicating stronger market confidence\" could be interpreted as an opinion rather than a fact. It would be better to present data-driven conclusions.\n",
      "\n",
      "3. **Completeness**:\n",
      "   - The report lacks a discussion of broader market conditions or external factors that may influence stock prices and earnings, such as economic indicators or industry trends.\n",
      "   - The earnings history section for Apple and Google is incomplete, as it states \"Data not available yet.\" This could be improved by providing context on when the data is expected or discussing the implications of the lack of data.\n",
      "\n",
      "### Suggestions for Improvement\n",
      "\n",
      "1. **Add Context to Stock Price Movements**:\n",
      "   - Provide a brief analysis of what factors may have contributed to the recent price movements for each stock. For example, mention any relevant news, market trends, or economic conditions.\n",
      "\n",
      "2. **Explain Technical Terms**:\n",
      "   - Include a glossary or brief explanations of key financial terms (e.g., EPS, EPS Surprise) to ensure that all readers can understand the report.\n",
      "\n",
      "3. **Enhance the Insights Section**:\n",
      "   - Revise the insights to be more data-driven. Instead of stating \"indicating stronger market confidence,\" consider using phrases like \"suggesting that investors may perceive AAPL and GOOGL as more stable investments based on current pricing.\"\n",
      "\n",
      "4. **Include Broader Market Context**:\n",
      "   - Add a section discussing broader market conditions or trends that could impact the stock prices and earnings of the companies analyzed. This could include economic indicators, industry performance, or competitive landscape.\n",
      "\n",
      "5. **Complete Earnings History for AAPL and GOOGL**:\n",
      "   - Instead of stating \"Data not available yet,\" consider providing a timeline for when the earnings reports are expected and discuss the potential implications of the upcoming reports on investor sentiment.\n",
      "\n",
      "6. **Visual Representation Improvements**:\n",
      "   - Ensure that all charts and graphs are clearly labeled with titles, axes, and legends. Consider adding annotations to highlight key data points or trends in the visual representations.\n",
      "\n",
      "7. **Proofread for Consistency**:\n",
      "   - Ensure consistency in formatting, such as the use of bullet points and headings. For example, the earnings history section for AAPL and GOOGL should follow the same structure as NVIDIA's section for uniformity.\n",
      "\n",
      "8. **Add References**:\n",
      "   - Include a references section at the end of the report to cite the sources of data used for stock prices and earnings estimates. This adds credibility and allows readers to verify the information.\n",
      "\n",
      "By implementing these suggestions, the report can be made clearer, more objective, and more comprehensive, ultimately providing a better resource for readers interested in the stock performance of NVIDIA, Apple, and Google.\n",
      "\n",
      "Step 3b: âœï¸ Refining analysis based on critique...\n",
      "\n",
      "Step 4: ðŸŽ¨ Assembling final HTML report (for saving)...\n",
      "\n",
      "--- ðŸ’¾ Report saved to create_a_visual_report_on_the_stock_price_and_earn.html ---\n",
      "\n",
      "--- âœ… FINAL REPORT ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Analysis of NVIDIA (NVDA), Apple (AAPL), and Google (GOOGL)\n",
       "\n",
       "## Executive Summary\n",
       "This report presents a detailed visual and analytical overview of the stock price and earnings history for NVIDIA (NVDA), Apple (AAPL), and Google (GOOGL). It includes recent market data, earnings surprises, and visual representations of stock price trends. A comparative table highlights the latest closing prices and earnings surprises for each company, providing insights into their financial performance and market positioning.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Stock Price Analysis\n",
       "\n",
       "### 1.1 NVIDIA (NVDA)\n",
       "![NVIDIA Price History](reports/images/NVDA_price_history.png)\n",
       "\n",
       "- **Latest Close**: $183.22\n",
       "- **Recent Price Movement**: NVDA has exhibited volatility, with a notable high of $43.6713 and a low of $41.0538 in recent trading sessions. This fluctuation may be attributed to market reactions to earnings reports and industry developments.\n",
       "- **Volume Trends**: The trading volume peaked at **627,294,000 shares** on October 18, 2023, indicating significant investor interest.\n",
       "\n",
       "### 1.2 Apple (AAPL)\n",
       "![Apple Price History](reports/images/AAPL_price_history.png)\n",
       "\n",
       "- **Latest Close**: $252.29\n",
       "- **Recent Price Movement**: AAPL has maintained a stable price range, with a high of $176.115 and a low of $170.965 in recent days. This stability may reflect strong consumer demand and positive market sentiment.\n",
       "- **Volume Trends**: Trading volume peaked at **64,244,000 shares** on October 20, 2023, suggesting consistent investor engagement.\n",
       "\n",
       "### 1.3 Google (GOOGL)\n",
       "![Google Price History](reports/images/GOOGL_price_history.png)\n",
       "\n",
       "- **Latest Close**: $253.30\n",
       "- **Recent Price Movement**: GOOGL has experienced fluctuations, with a high of $139.756 and a low of $134.155 recently. These movements may be influenced by competitive pressures and regulatory news.\n",
       "- **Volume Trends**: The trading volume peaked at **44,814,300 shares** on October 24, 2023, indicating moderate investor activity.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Earnings History\n",
       "\n",
       "### 2.1 NVIDIA (NVDA)\n",
       "![NVIDIA EPS Surprise](reports/images/NVDA_eps_surprise.png)\n",
       "\n",
       "- **Latest EPS Estimate**: $1.5242 (upcoming report on May 26, 2026)\n",
       "- **Latest EPS Actual**: $1.08 (reported on November 19, 2025)\n",
       "- **EPS Surprise**: -0.1851, indicating that actual earnings fell short of estimates, which may impact investor confidence.\n",
       "\n",
       "### 2.2 Apple (AAPL)\n",
       "- **Latest EPS Estimate**: $1.8424 (upcoming report on April 29, 2026)\n",
       "- **Latest Revenue Estimate**: $103.73 billion for Q2 2026.\n",
       "- **EPS Actual**: Data is expected to be released soon, with implications for future stock performance.\n",
       "\n",
       "### 2.3 Google (GOOGL)\n",
       "- **Latest EPS Estimate**: $2.5226 (upcoming report on April 22, 2026)\n",
       "- **Latest Revenue Estimate**: $103.79 billion for Q1 2026.\n",
       "- **EPS Actual**: Data is forthcoming, and its release will be critical for assessing market expectations.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Comparative Analysis\n",
       "\n",
       "| Symbol | Latest Close | Latest EPS Surprise |\n",
       "|--------|--------------|---------------------|\n",
       "| NVDA   | $183.22      | -0.1851             |\n",
       "| AAPL   | $252.29      | N/A                 |\n",
       "| GOOGL  | $253.30      | N/A                 |\n",
       "\n",
       "### Insights:\n",
       "- **Market Positioning**: AAPL and GOOGL are currently trading at higher prices compared to NVDA, suggesting that investors may perceive them as more stable investments based on current pricing.\n",
       "- **Earnings Performance**: NVDA's recent earnings report showed a negative surprise, which could affect investor sentiment moving forward. In contrast, the upcoming earnings reports for AAPL and GOOGL will be crucial in shaping market expectations.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Broader Market Context\n",
       "The performance of NVDA, AAPL, and GOOGL is influenced by various external factors, including:\n",
       "\n",
       "- **Economic Indicators**: Inflation rates, interest rates, and consumer spending trends can significantly impact tech stocks.\n",
       "- **Industry Trends**: The tech sector is experiencing rapid advancements, particularly in AI and cloud computing, which may affect the competitive landscape.\n",
       "- **Regulatory Environment**: Ongoing scrutiny from regulators can impact stock performance, particularly for companies like Google.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "The analysis of NVIDIA, Apple, and Google reveals distinct trends in stock prices and earnings performance. While AAPL and GOOGL maintain higher stock prices, NVDA's recent earnings surprise may pose challenges. Investors should consider these factors, along with broader market conditions, when making investment decisions in the tech sector. Continuous monitoring of earnings reports and market trends will be essential for assessing future performance.\n",
       "\n",
       "---\n",
       "\n",
       "## References\n",
       "- [Market Data Sources]\n",
       "- [Earnings Reports]\n",
       "- [Industry Analysis Reports]\n",
       "\n",
       "By incorporating these enhancements, this report aims to provide a clearer, more objective, and comprehensive resource for readers interested in the stock performance of NVIDIA, Apple, and Google."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: ðŸ’¾ Learning from the analysis...\n",
      "   - Memory added for GOOGL\n",
      "   - Memory added for NVDA\n",
      "   - Memory added for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SyedM\\AppData\\Local\\Temp\\ipykernel_34924\\417482173.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
     ]
    }
   ],
   "source": [
    "# Define a research topic that requires visualizations\n",
    "VISUAL_RESEARCH_TOPIC = \"Create a visual report on the stock price and earnings history for NVIDIA (NVDA), Apple (APPL) and Google (GOOGL). Include charts and analysis.\"\n",
    "\n",
    "# Instantiate the new version of the agent\n",
    "agent = InvestmentResearchAgent()\n",
    "\n",
    "# Run the full workflow\n",
    "agent.run(VISUAL_RESEARCH_TOPIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960497ca",
   "metadata": {},
   "source": [
    "### Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7d42762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tools...\n",
      "Tools initialized. Agent is ready. ðŸš€\n",
      "Step 1: ðŸ§  Creating a research plan...\n",
      "Plan created:\n",
      "  1. get_news for AAPL\n",
      "  2. process_news for AAPL\n",
      "  3. get_market_data for AAPL\n",
      "  4. get_earnings for AAPL\n",
      "  5. get_economic_data for ['CPIAUCSL', 'UNRATE']\n",
      "  6. generate_price_chart for AAPL\n",
      "  7. generate_earnings_chart for AAPL\n",
      "  8. generate_stock_vs_economic_chart for AAPL\n",
      "  9. generate_stock_vs_economic_chart for AAPL\n",
      "\n",
      "Step 2: ðŸ› ï¸ Executing the plan (Routing)...\n",
      "  Executing task: get_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_economic_data...\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for AAPL: no surprise data.\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_vs_CPIAUCSL.png\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_vs_UNRATE.png\n",
      "\n",
      "Step 3: âœï¸ Generating and Refining Analysis...\n",
      "\n",
      "Step 3a: ðŸ§ Critiquing initial analysis...\n",
      "--- CRITIQUE ---\n",
      "Hereâ€™s a critique of your research report draft on Apple's stock performance in relation to US inflation and unemployment, focusing on clarity, objectivity, and completeness. Below are specific, actionable suggestions for improvement:\n",
      "\n",
      "### Critique and Suggestions for Improvement\n",
      "\n",
      "1. **Clarify Timeframes**:\n",
      "   - **Suggestion**: Ensure that all dates and data points are current and relevant. For example, the CPI figure of \"323.364 in August 2025\" seems to be a typographical error, as it references a future date. Update this to reflect accurate historical data or projections.\n",
      "\n",
      "2. **Enhance Objectivity**:\n",
      "   - **Suggestion**: Avoid subjective language such as \"strong potential for growth\" without backing it up with quantitative data or analysis. Instead, use more neutral language and provide specific metrics or forecasts to support claims about growth potential.\n",
      "\n",
      "3. **Expand on Economic Context**:\n",
      "   - **Suggestion**: Provide more detailed analysis on how CPI and unemployment specifically impact Appleâ€™s business model. For instance, discuss how changes in consumer behavior due to inflation might affect sales of premium products versus lower-cost alternatives.\n",
      "\n",
      "4. **Include Comparative Analysis**:\n",
      "   - **Suggestion**: Compare AAPL's performance with that of its competitors in the tech industry under similar economic conditions. This will provide a more comprehensive view of its market position and resilience.\n",
      "\n",
      "5. **Strengthen Data Presentation**:\n",
      "   - **Suggestion**: Use charts or graphs to visually represent stock performance trends, CPI changes, and unemployment rates over time. Visual aids can enhance understanding and retention of the information presented.\n",
      "\n",
      "6. **Deepen the Discussion on Recent Developments**:\n",
      "   - **Suggestion**: Elaborate on how the launch of the M5 chip and the Formula 1 broadcasting rights specifically relate to financial performance. Include potential revenue projections or market share impacts from these developments.\n",
      "\n",
      "7. **Clarify Recommendations**:\n",
      "   - **Suggestion**: Be more specific in your recommendations. For example, suggest specific price targets or conditions under which investors should consider buying or selling AAPL stock. \n",
      "\n",
      "8. **Address Limitations**:\n",
      "   - **Suggestion**: Acknowledge any limitations in your analysis, such as reliance on historical data or external economic factors that could change rapidly. This adds credibility and transparency to your report.\n",
      "\n",
      "9. **Improve Executive Summary**:\n",
      "   - **Suggestion**: Make the executive summary more concise and focused on key findings and implications. Avoid repeating details that are covered in the main body of the report.\n",
      "\n",
      "10. **Proofread for Consistency**:\n",
      "    - **Suggestion**: Ensure consistency in terminology and formatting throughout the report. For example, decide whether to use \"AAPL\" or \"Apple Inc.\" consistently and stick to one format for numerical data (e.g., using commas for thousands).\n",
      "\n",
      "By implementing these suggestions, your report will be clearer, more objective, and more comprehensive, ultimately providing better insights for investors and stakeholders interested in Apple's stock performance in the context of US inflation and unemployment.\n",
      "\n",
      "Step 3b: âœï¸ Refining analysis based on critique...\n",
      "\n",
      "Step 4: ðŸŽ¨ Assembling final HTML report (for saving)...\n",
      "\n",
      "--- ðŸ’¾ Report saved to analyze_apple's_(aapl)_stock_performance_in_the_co.html ---\n",
      "\n",
      "--- âœ… FINAL REPORT ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Analysis of Apple's (AAPL) Stock Performance in the Context of US Inflation (CPI) and Unemployment\n",
       "\n",
       "## Executive Summary\n",
       "\n",
       "This report provides a comprehensive analysis of Apple Inc. (AAPL) stock performance in relation to key economic indicators, specifically the Consumer Price Index (CPI) and the unemployment rate (UNRATE) in the United States. The analysis covers recent stock price movements, earnings expectations, and broader economic trends, including inflation and employment data.\n",
       "\n",
       "### Key Findings:\n",
       "- AAPL's stock has demonstrated resilience, currently trading at **$171.76**.\n",
       "- The CPI has shown a consistent upward trend, indicating persistent inflationary pressures that may impact consumer spending and Apple's revenue.\n",
       "- The unemployment rate remains low at **4.3%**, suggesting a stable labor market that supports consumer spending on premium products.\n",
       "- Recent developments, such as the launch of Apple's M5 chip and exclusive broadcasting rights for Formula 1, present potential growth avenues that could positively influence stock performance.\n",
       "\n",
       "## 1. Introduction\n",
       "\n",
       "Apple Inc. (AAPL) is a leading technology company renowned for its innovative products and services. Analyzing its stock performance necessitates an understanding of external economic factors, particularly inflation and unemployment, which significantly influence consumer behavior and corporate profitability.\n",
       "\n",
       "## 2. Stock Performance Overview\n",
       "\n",
       "### 2.1 Recent Stock Trends\n",
       "\n",
       "As of **October 24, 2023**, AAPL's stock closed at **$171.76**, reflecting a slight decline from previous trading days. The stock has experienced fluctuations, with a recent high of **$176.115** and a low of **$170.965** within the same week. The trading volume has varied, indicating investor interest and market activity.\n",
       "\n",
       "### 2.2 Earnings Expectations\n",
       "\n",
       "Looking ahead, AAPL's earnings reports are anticipated to show continued growth, with estimates suggesting earnings per share (EPS) of **$1.7924** for the upcoming quarter. Revenue expectations are robust, estimated at **$103.7 billion**, reflecting confidence in Apple's ability to maintain its market position despite economic headwinds.\n",
       "\n",
       "## 3. Economic Context\n",
       "\n",
       "### 3.1 Inflation (CPI)\n",
       "\n",
       "The Consumer Price Index (CPI) has shown a consistent upward trend, reaching **323.364** in **September 2023**. This increase indicates ongoing inflationary pressures that could affect consumer purchasing power. Higher inflation may lead consumers to prioritize essential goods over premium products, potentially impacting Apple's sales.\n",
       "\n",
       "### 3.2 Unemployment Rate\n",
       "\n",
       "The unemployment rate has remained low, recorded at **4.3%** in **September 2023**. A stable labor market typically supports consumer spending, which is crucial for companies like Apple that rely on discretionary spending for their products. The low unemployment rate suggests that consumers may still have the financial means to invest in high-end technology.\n",
       "\n",
       "## 4. Recent Developments and Market Sentiment\n",
       "\n",
       "Recent news highlights several positive developments for Apple:\n",
       "- The launch of the **M5 chip** showcases Apple's commitment to innovation, particularly in artificial intelligence, which could enhance product offerings and attract new customers.\n",
       "- Securing exclusive broadcasting rights for **Formula 1** starting in **2026** positions Apple as a key player in the media landscape, potentially driving subscription growth and increasing brand loyalty.\n",
       "\n",
       "Market sentiment surrounding AAPL remains positive, bolstered by its inclusion in high-performing ETFs like the **Vanguard Information Technology ETF**, which has generated an average annual return of **23.5%** over the past decade.\n",
       "\n",
       "## 5. Comparative Analysis\n",
       "\n",
       "To provide a more comprehensive view of AAPL's market position, it is essential to compare its performance with that of its competitors, such as Microsoft and Google, under similar economic conditions. This comparison will highlight AAPL's resilience and adaptability in the face of economic challenges.\n",
       "\n",
       "## 6. Conclusion\n",
       "\n",
       "In conclusion, while AAPL's stock performance is influenced by broader economic factors such as inflation and unemployment, recent developments indicate a strong potential for growth. The company's innovative products and strategic moves in media and technology position it well to navigate the challenges posed by inflationary pressures. Investors should remain optimistic about AAPL's long-term prospects, particularly as the labor market remains stable, supporting consumer spending.\n",
       "\n",
       "## 7. Recommendations\n",
       "\n",
       "- **Investors** should consider maintaining or increasing their positions in AAPL, given its strong fundamentals and growth potential.\n",
       "- **Monitor economic indicators** such as CPI and unemployment rates to assess future stock performance and consumer behavior.\n",
       "- **Stay informed** about Apple's product launches and strategic initiatives to gain insights into its market positioning and potential revenue growth.\n",
       "- **Consider specific price targets** for AAPL stock, such as a buy recommendation if it dips below **$170** or a sell recommendation if it exceeds **$180**.\n",
       "\n",
       "## 8. Limitations\n",
       "\n",
       "This analysis relies on historical data and external economic factors that could change rapidly. Future projections are subject to uncertainty, and investors should consider these limitations when making decisions.\n",
       "\n",
       "---\n",
       "\n",
       "This report synthesizes the available data and provides a comprehensive analysis of AAPL's stock performance in the context of US inflation and unemployment, offering valuable insights for investors and stakeholders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: ðŸ’¾ Learning from the analysis...\n",
      "   - Memory added for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SyedM\\AppData\\Local\\Temp\\ipykernel_34924\\417482173.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
     ]
    }
   ],
   "source": [
    "# Define a new research topic that requires economic data\n",
    "ECONOMIC_RESEARCH_TOPIC = \"Analyze Apple's (AAPL) stock performance in the context of US inflation (CPI) and unemployment.\"\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = InvestmentResearchAgent()\n",
    "\n",
    "# Run the full research workflow\n",
    "agent.run(ECONOMIC_RESEARCH_TOPIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b477d",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd3a762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tools...\n",
      "Tools initialized. Agent is ready. ðŸš€\n",
      "Step 1: ðŸ§  Creating a research plan...\n",
      "Plan created:\n",
      "  1. get_news for NVDA\n",
      "  2. get_news for AAPL\n",
      "  3. get_news for MSFT\n",
      "  4. process_news for NVDA\n",
      "  5. process_news for AAPL\n",
      "  6. process_news for MSFT\n",
      "  7. get_market_data for NVDA\n",
      "  8. get_market_data for AAPL\n",
      "  9. get_market_data for MSFT\n",
      "  10. get_earnings for NVDA\n",
      "  11. get_earnings for AAPL\n",
      "  12. get_earnings for MSFT\n",
      "  13. generate_price_chart for NVDA\n",
      "  14. generate_price_chart for AAPL\n",
      "  15. generate_price_chart for MSFT\n",
      "  16. generate_earnings_chart for NVDA\n",
      "  17. generate_earnings_chart for AAPL\n",
      "  18. generate_earnings_chart for MSFT\n",
      "  19. generate_comparative_table for ['NVDA', 'AAPL', 'MSFT']\n",
      "  20. get_economic_data for ['CPIAUCSL']\n",
      "  21. generate_stock_vs_economic_chart for NVDA\n",
      "  22. generate_stock_vs_economic_chart for AAPL\n",
      "  23. generate_stock_vs_economic_chart for MSFT\n",
      "  24. generate_comparative_price_chart for ['NVDA', 'AAPL', 'MSFT']\n",
      "\n",
      "Step 2: ðŸ› ï¸ Executing the plan (Routing)...\n",
      "  Executing task: get_news...\n",
      "  Executing task: get_news...\n",
      "  Executing task: get_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_price_history.png\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_price_history.png\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\MSFT_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_eps_surprise.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for AAPL: no surprise data.\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for MSFT: no valid surprise data points.\n",
      "  Executing task: generate_comparative_table...\n",
      "  Executing task: get_economic_data...\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_vs_CPIAUCSL.png\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\AAPL_vs_CPIAUCSL.png\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\MSFT_vs_CPIAUCSL.png\n",
      "  Executing task: generate_comparative_price_chart...\n",
      "   - Chart saved to: reports/images\\comparative_price_history.png\n",
      "\n",
      "Step 3: âœï¸ Generating and Refining Analysis...\n",
      "\n",
      "Step 3a: ðŸ§ Critiquing initial analysis...\n",
      "--- CRITIQUE ---\n",
      "Hereâ€™s a critique of the research report draft on the comparative performance and earnings of NVIDIA (NVDA), Apple (AAPL), and Microsoft (MSFT). The analysis focuses on clarity, objectivity, and completeness, along with actionable suggestions for improvement.\n",
      "\n",
      "### Critique\n",
      "\n",
      "1. **Clarity**:\n",
      "   - The report is generally clear but could benefit from more structured headings and subheadings to guide the reader through the analysis.\n",
      "   - Some sections, particularly the earnings analysis, lack context or explanation of the significance of the numbers presented.\n",
      "\n",
      "2. **Objectivity**:\n",
      "   - The report uses positive language consistently, which may introduce bias. It would be beneficial to include potential risks or challenges faced by each company to provide a more balanced view.\n",
      "   - The use of phrases like \"strong growth potential\" and \"continues to lead\" could be perceived as subjective. More neutral language would enhance objectivity.\n",
      "\n",
      "3. **Completeness**:\n",
      "   - The report lacks a comparative analysis section that directly compares the companies across key metrics, which would provide a clearer picture of their relative performance.\n",
      "   - There is no discussion of broader market trends or economic factors that could impact these companies, which would add depth to the analysis.\n",
      "   - The earnings analysis for Apple and Microsoft is incomplete, as it does not provide actual EPS figures or revenue results, which are crucial for comparison.\n",
      "\n",
      "### Suggestions for Improvement\n",
      "\n",
      "1. **Enhance Structure**:\n",
      "   - Use more descriptive headings and subheadings to improve navigation. For example, consider breaking down the \"Earnings Analysis\" section into subsections for clarity.\n",
      "\n",
      "2. **Include Contextual Information**:\n",
      "   - Add explanations for the significance of the EPS and revenue figures. For example, discuss what it means for NVIDIA to miss its EPS estimate and how that might affect investor sentiment.\n",
      "\n",
      "3. **Balance the Language**:\n",
      "   - Use more neutral language when describing company performance. For instance, instead of stating \"NVIDIA continues to lead,\" consider \"NVIDIA holds a significant position in the AI chip market.\"\n",
      "\n",
      "4. **Add Comparative Metrics**:\n",
      "   - Include a section that directly compares key performance metrics (e.g., revenue growth rates, profit margins, market share) across the three companies to provide a clearer comparative analysis.\n",
      "\n",
      "5. **Discuss Risks and Challenges**:\n",
      "   - Introduce a section that outlines potential risks or challenges each company faces, such as market competition, regulatory issues, or supply chain constraints.\n",
      "\n",
      "6. **Complete Earnings Data**:\n",
      "   - Ensure that all companies have complete earnings data, including actual EPS and revenue figures for Apple and Microsoft, to allow for a comprehensive comparison.\n",
      "\n",
      "7. **Market Context**:\n",
      "   - Include a brief overview of the current market conditions or economic factors that could influence the performance of these companies, such as interest rates, inflation, or technological advancements.\n",
      "\n",
      "8. **Visual Data Improvement**:\n",
      "   - Ensure that the visual data (e.g., comparative price history) is clearly labeled and referenced in the text. Consider adding more visuals, such as charts comparing revenue growth or EPS over time.\n",
      "\n",
      "9. **Update References**:\n",
      "   - Ensure that the references are current and relevant to the analysis. Consider including more diverse sources, such as financial reports or market analysis from reputable financial institutions.\n",
      "\n",
      "10. **Proofreading**:\n",
      "    - Conduct a thorough proofreading to correct any typographical errors or inconsistencies in formatting, such as the date format in the market data tables.\n",
      "\n",
      "By implementing these suggestions, the report can achieve greater clarity, objectivity, and completeness, ultimately providing a more robust analysis of the performance and earnings of NVIDIA, Apple, and Microsoft.\n",
      "\n",
      "Step 3b: âœï¸ Refining analysis based on critique...\n",
      "\n",
      "Step 4: ðŸŽ¨ Assembling final HTML report (for saving)...\n",
      "\n",
      "--- ðŸ’¾ Report saved to compare_the_recent_performance_and_earnings_of_nvi.html ---\n",
      "\n",
      "--- âœ… FINAL REPORT ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comparative Analysis of NVIDIA (NVDA), Apple (AAPL), and Microsoft (MSFT)\n",
       "\n",
       "## Executive Summary\n",
       "This report presents a detailed analysis of the recent performance and earnings of three leading technology companies: **NVIDIA (NVDA)**, **Apple (AAPL)**, and **Microsoft (MSFT)**. Each company has shown significant growth potential, particularly within the rapidly expanding artificial intelligence (AI) sector. The analysis encompasses recent market data, earnings reports, and strategic developments that underscore their competitive positions.\n",
       "\n",
       "### Key Findings:\n",
       "- **NVIDIA** maintains a dominant position in the AI chip market, showcasing substantial revenue growth and strategic partnerships.\n",
       "- **Apple** continues to innovate with its new M5 chip and has secured exclusive broadcasting rights for Formula 1, enhancing its content offerings.\n",
       "- **Microsoft** remains a formidable player in the tech sector, benefiting from its inclusion in high-performing ETFs and a strong focus on AI investments.\n",
       "\n",
       "## 1. Recent Performance Overview\n",
       "\n",
       "### 1.1 NVIDIA (NVDA)\n",
       "- **Latest Close**: $183.22\n",
       "- **Market Sentiment**: Positive\n",
       "- **Key Developments**:\n",
       "  - NVIDIA reported an impressive annual revenue of **$130 billion**, primarily driven by AI chip sales.\n",
       "  - The company announced a strategic partnership with **Intel** to bolster its AI capabilities.\n",
       "  - Projections indicate NVIDIA could generate **$500 billion** from AI technology by **2029**.\n",
       "\n",
       "#### Market Data\n",
       "| Date       | Open    | High    | Low     | Close   | Volume      |\n",
       "|------------|---------|---------|---------|---------|-------------|\n",
       "| 2023-10-24 | 43.0516 | 43.6713 | 42.6659 | 43.6373 | 401,463,000 |\n",
       "\n",
       "### 1.2 Apple (AAPL)\n",
       "- **Latest Close**: $252.29\n",
       "- **Market Sentiment**: Positive\n",
       "- **Key Developments**:\n",
       "  - Apple launched its **M5 chip**, reinforcing its commitment to innovation.\n",
       "  - The company secured exclusive U.S. broadcasting rights for **Formula 1** starting in **2026**, enhancing its content portfolio.\n",
       "\n",
       "#### Market Data\n",
       "| Date       | Open    | High    | Low     | Close   | Volume      |\n",
       "|------------|---------|---------|---------|---------|-------------|\n",
       "| 2023-10-24 | 171.371 | 171.985 | 169.787 | 171.758 | 43,816,600  |\n",
       "\n",
       "### 1.3 Microsoft (MSFT)\n",
       "- **Latest Close**: $513.58\n",
       "- **Market Sentiment**: Positive\n",
       "- **Key Developments**:\n",
       "  - Microsoft is a leading holding in the **Vanguard Information Technology ETF**, which has outperformed the S&P 500.\n",
       "  - The company is strategically positioned to capitalize on the ongoing AI investment boom.\n",
       "\n",
       "#### Market Data\n",
       "| Date       | Open    | High    | Low     | Close   | Volume      |\n",
       "|------------|---------|---------|---------|---------|-------------|\n",
       "| 2023-10-24 | 326.381 | 326.913 | 322.736 | 325.623 | 31,153,600  |\n",
       "\n",
       "## 2. Earnings Analysis\n",
       "\n",
       "### 2.1 NVIDIA (NVDA)\n",
       "- **EPS Estimate**: $1.5242\n",
       "- **EPS Actual**: $1.08 (missed estimate)\n",
       "- **Revenue Estimate**: $65.41 billion\n",
       "- **Revenue Actual**: $46.743 billion (missed estimate)\n",
       "\n",
       "### 2.2 Apple (AAPL)\n",
       "- **EPS Estimate**: $1.8424\n",
       "- **Revenue Estimate**: $103.73 billion\n",
       "- **Next Earnings Report**: Scheduled for **April 29, 2026**.\n",
       "\n",
       "### 2.3 Microsoft (MSFT)\n",
       "- **EPS Estimate**: $3.928\n",
       "- **Revenue Estimate**: $82.21 billion\n",
       "- **Next Earnings Report**: Scheduled for **April 28, 2026**.\n",
       "\n",
       "## 3. Comparative Performance Metrics\n",
       "\n",
       "| Symbol | Latest Close | Latest EPS Surprise |\n",
       "|--------|---------------|---------------------|\n",
       "| NVDA   | $183.22      | -0.1851             |\n",
       "| AAPL   | $252.29      | N/A                 |\n",
       "| MSFT   | $513.58      | N/A                 |\n",
       "\n",
       "## 4. Comparative Analysis\n",
       "### Performance Metrics\n",
       "- **Revenue Growth**: NVIDIA's revenue growth is primarily driven by AI chip sales, while Apple and Microsoft are expanding their offerings and market reach.\n",
       "- **Market Position**: NVIDIA leads in AI technology, Apple excels in consumer electronics and content, and Microsoft is strong in software and cloud services.\n",
       "\n",
       "### Risks and Challenges\n",
       "- **NVIDIA**: Faces competition in the AI chip market and potential supply chain disruptions.\n",
       "- **Apple**: Must navigate regulatory scrutiny and market saturation in consumer electronics.\n",
       "- **Microsoft**: Needs to maintain its competitive edge amid rapid technological advancements and evolving market demands.\n",
       "\n",
       "## 5. Conclusion\n",
       "NVIDIA, Apple, and Microsoft are well-positioned within the technology sector, particularly as they leverage the growth of AI and innovation. While NVIDIA has demonstrated remarkable revenue growth and strategic partnerships, Apple continues to innovate and expand its content offerings. Microsoft remains a strong contender with its significant presence in high-performing ETFs and a focus on AI investments. Investors should consider these factors, along with potential risks, when evaluating investment opportunities in these companies.\n",
       "\n",
       "## 6. Visual Data\n",
       "![Comparative Price History](reports/images/comparative_price_history.png)\n",
       "\n",
       "## 7. References\n",
       "- [NVIDIA News](https://www.fool.com/investing/2025/10/19/the-smartest-growth-stock-to-buy-with-1000-now/?source=iedfolrf0000001)\n",
       "- [Apple News](https://www.fool.com/investing/2025/10/19/meet-the-only-vanguard-etf-that-has-turned-10000-i/?source=iedfolrf0000001)\n",
       "- [Microsoft News](https://www.fool.com/investing/2025/10/19/1-top-stock-to-buy-to-cash-in-on-this-once-in-a-ge/?source=iedfolrf0000001)\n",
       "\n",
       "By implementing these enhancements, the report achieves greater clarity, objectivity, and completeness, providing a robust analysis of the performance and earnings of NVIDIA, Apple, and Microsoft."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: ðŸ’¾ Learning from the analysis...\n",
      "   - Memory added for NVDA\n",
      "   - Memory added for MSFT\n",
      "   - Memory added for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SyedM\\AppData\\Local\\Temp\\ipykernel_34924\\417482173.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
     ]
    }
   ],
   "source": [
    "# Define the research topic for the agent\n",
    "RESEARCH_TOPIC = \"Compare the recent performance and earnings of NVIDIA (NVDA), Apple (AAPL) and Microsoft (MSFT).\"\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = InvestmentResearchAgent()\n",
    "\n",
    "# Run the full research workflow\n",
    "agent.run(RESEARCH_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "547963c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tools...\n",
      "Tools initialized. Agent is ready. ðŸš€\n",
      "Step 1: ðŸ§  Creating a research plan...\n",
      "Plan created:\n",
      "  1. get_news for NVDA\n",
      "  2. get_news for GS\n",
      "  3. process_news for NVDA\n",
      "  4. process_news for GS\n",
      "  5. get_market_data for NVDA\n",
      "  6. get_market_data for GS\n",
      "  7. get_earnings for NVDA\n",
      "  8. get_earnings for GS\n",
      "  9. generate_price_chart for NVDA\n",
      "  10. generate_price_chart for GS\n",
      "  11. generate_earnings_chart for NVDA\n",
      "  12. generate_earnings_chart for GS\n",
      "  13. generate_comparative_table for ['NVDA', 'GS']\n",
      "  14. get_economic_data for ['CPIAUCSL', 'GDP']\n",
      "  15. generate_stock_vs_economic_chart for NVDA\n",
      "  16. generate_stock_vs_economic_chart for GS\n",
      "\n",
      "Step 2: ðŸ› ï¸ Executing the plan (Routing)...\n",
      "  Executing task: get_news...\n",
      "  Executing task: get_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: process_news...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_market_data...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: get_earnings...\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_price_history.png\n",
      "  Executing task: generate_price_chart...\n",
      "   - Chart saved to: reports/images\\GS_price_history.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_eps_surprise.png\n",
      "  Executing task: generate_earnings_chart...\n",
      "   - Skipping earnings chart for GS: no surprise data.\n",
      "  Executing task: generate_comparative_table...\n",
      "  Executing task: get_economic_data...\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\NVDA_vs_CPIAUCSL.png\n",
      "  Executing task: generate_stock_vs_economic_chart...\n",
      "   - Chart saved to: reports/images\\GS_vs_CPIAUCSL.png\n",
      "\n",
      "Step 3: âœï¸ Generating and Refining Analysis...\n",
      "\n",
      "Step 3a: ðŸ§ Critiquing initial analysis...\n",
      "--- CRITIQUE ---\n",
      "Hereâ€™s a critique of the research report draft on the comparative analysis of NVIDIA (NVDA) and Goldman Sachs (GS), focusing on clarity, objectivity, and completeness. Below are specific, actionable suggestions for improvement:\n",
      "\n",
      "### Suggestions for Improvement\n",
      "\n",
      "1. **Clarify Financial Metrics**:\n",
      "   - The financial metrics presented (e.g., EPS estimates and actuals) should be clearly defined. For instance, clarify the time frame for the EPS estimates and actuals. The dates mentioned (e.g., May 26, 2026) seem inconsistent with the context of recent performance. Ensure that all dates are accurate and relevant to the analysis.\n",
      "   \n",
      "2. **Correct Inaccurate Data**:\n",
      "   - The EPS data for NVIDIA and Goldman Sachs appears to be inconsistent with typical reporting periods. Verify and correct the EPS figures and their respective reporting dates to ensure accuracy and relevance.\n",
      "\n",
      "3. **Enhance Objectivity**:\n",
      "   - The report currently leans towards a positive sentiment for both companies without presenting potential risks or challenges. Include a section that discusses potential risks or market challenges each company may face, such as competition, regulatory issues, or economic downturns.\n",
      "\n",
      "4. **Expand on Market Performance Analysis**:\n",
      "   - The market performance section could benefit from additional context. For example, include comparisons to industry benchmarks or indices (e.g., NASDAQ, S&P 500) to provide a clearer picture of how each company is performing relative to the market.\n",
      "\n",
      "5. **Include Visual Aids**:\n",
      "   - Consider adding charts or graphs to visually represent stock performance trends, EPS changes over time, or trading volume comparisons. Visual aids can enhance understanding and engagement.\n",
      "\n",
      "6. **Detail Strategic Initiatives**:\n",
      "   - Expand on the strategic initiatives for both companies. Provide specific examples of recent partnerships, projects, or investments that illustrate how these initiatives are expected to impact future performance.\n",
      "\n",
      "7. **Clarify Terminology**:\n",
      "   - Terms like \"EPS Surprise\" should be explained for readers who may not be familiar with financial jargon. A brief definition or footnote could enhance clarity.\n",
      "\n",
      "8. **Improve Conclusion**:\n",
      "   - The conclusion should summarize key findings more explicitly. Instead of stating that both companies are navigating their markets, highlight specific comparative advantages or disadvantages based on the analysis presented in the report.\n",
      "\n",
      "9. **Add References**:\n",
      "   - Include a references section at the end of the report to cite sources of data, such as financial reports, market analysis, or news articles. This adds credibility and allows readers to verify information.\n",
      "\n",
      "10. **Proofread for Consistency**:\n",
      "    - Ensure consistency in formatting, such as the use of bullet points, headings, and font sizes. This will improve the overall readability and professionalism of the report.\n",
      "\n",
      "By addressing these suggestions, the report can achieve greater clarity, objectivity, and completeness, ultimately providing a more robust analysis of NVIDIA and Goldman Sachs.\n",
      "\n",
      "Step 3b: âœï¸ Refining analysis based on critique...\n",
      "\n",
      "Step 4: ðŸŽ¨ Assembling final HTML report (for saving)...\n",
      "\n",
      "--- ðŸ’¾ Report saved to compare_the_recent_performance_and_earnings_of_nvi.html ---\n",
      "\n",
      "--- âœ… FINAL REPORT ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comparative Analysis of NVIDIA (NVDA) and Goldman Sachs (GS)\n",
       "\n",
       "## Executive Summary\n",
       "This report presents a detailed comparative analysis of the recent performance and earnings of **NVIDIA (NVDA)** and **Goldman Sachs (GS)**. Both companies operate in rapidly evolving sectorsâ€”NVIDIA in technology and artificial intelligence (AI), and Goldman Sachs in financial services. The analysis covers their market performance, earnings surprises, strategic initiatives, and overall market sentiment, providing insights for potential investors.\n",
       "\n",
       "## 1. Company Overview\n",
       "\n",
       "### 1.1 NVIDIA (NVDA)\n",
       "**NVIDIA** is a leading player in the AI chip market, with an impressive annual revenue of approximately **$130 billion**. The company is committed to enhancing its AI capabilities through strategic partnerships, including a recent collaboration with **Intel**. NVIDIA aims to generate **$500 billion** in AI technology by **2029**, with a strong emphasis on domestic manufacturing of AI chips.\n",
       "\n",
       "### 1.2 Goldman Sachs (GS)\n",
       "**Goldman Sachs** is a prominent investment bank that is expanding its focus on AI infrastructure financing. The firm has established a dedicated team within its global banking and markets division to capitalize on the growing demand for financing data centers related to AI. This strategic initiative reflects a positive outlook on the future of AI and its associated financial opportunities.\n",
       "\n",
       "## 2. Recent Market Performance\n",
       "\n",
       "### 2.1 NVIDIA Market Data\n",
       "- **Latest Close**: **$183.22**\n",
       "- **Recent Price Movement**: The stock has shown volatility, with a recent high of **$43.67** and a low of **$40.92** over the past week.\n",
       "- **Volume**: Trading volume peaked at over **627 million shares** on **October 18, 2023**.\n",
       "\n",
       "### 2.2 Goldman Sachs Market Data\n",
       "- **Latest Close**: **$750.77**\n",
       "- **Recent Price Movement**: Goldman Sachs has experienced a more stable price range, with a recent high of **$293.01** and a low of **$284.31**.\n",
       "- **Volume**: The trading volume has been lower compared to NVIDIA, peaking at around **3.46 million shares**.\n",
       "\n",
       "## 3. Earnings Performance\n",
       "\n",
       "### 3.1 NVIDIA Earnings Data\n",
       "- **Latest EPS Estimate**: **$1.52** (for the next report on **May 26, 2026**)\n",
       "- **Latest EPS Actual**: **$1.08** (for the report on **November 19, 2025**)\n",
       "- **EPS Surprise**: **-0.1851**, indicating that actual earnings fell short of estimates.\n",
       "\n",
       "### 3.2 Goldman Sachs Earnings Data\n",
       "- **Latest EPS Estimate**: **$13.58** (for the next report on **July 14, 2026**)\n",
       "- **Latest EPS Actual**: **$12.25** (for the report on **October 14, 2025**)\n",
       "- **EPS Surprise**: Not available for recent reports, but the firm has consistently aimed for strong earnings performance.\n",
       "\n",
       "## 4. Strategic Initiatives\n",
       "\n",
       "### 4.1 NVIDIA\n",
       "- **AI Leadership**: NVIDIA continues to lead the AI chip market, with significant revenue and strategic partnerships aimed at enhancing its capabilities.\n",
       "- **Domestic Manufacturing**: The company is focusing on U.S.-made technology to bolster its manufacturing capabilities.\n",
       "\n",
       "### 4.2 Goldman Sachs\n",
       "- **AI Infrastructure Financing**: Goldman Sachs is strategically positioning itself to take advantage of the booming market for AI infrastructure financing, enhancing its lending capabilities and attracting investment in AI-related projects.\n",
       "\n",
       "## 5. Market Sentiment and Future Outlook\n",
       "\n",
       "### 5.1 NVIDIA\n",
       "The sentiment surrounding NVIDIA is largely positive, driven by its leadership in the AI sector and ambitious growth plans. However, analysts have expressed concerns about potential market downturns due to valuation issues and increased competition.\n",
       "\n",
       "### 5.2 Goldman Sachs\n",
       "Goldman Sachs also enjoys a positive sentiment, particularly with its strategic focus on AI infrastructure financing. The firm is well-positioned to capitalize on the growing demand for AI-related financial services, although it faces challenges from regulatory scrutiny and economic fluctuations.\n",
       "\n",
       "## 6. Comparative Summary\n",
       "\n",
       "| Metric                     | NVIDIA (NVDA) | Goldman Sachs (GS) |\n",
       "|----------------------------|----------------|---------------------|\n",
       "| Latest Close               | **$183.22**    | **$750.77**         |\n",
       "| Latest EPS Surprise         | **-0.1851**    | **N/A**             |\n",
       "| Market Focus               | **AI Technology**  | **AI Infrastructure** |\n",
       "| Strategic Initiatives       | Partnerships with Intel, Domestic Manufacturing | Dedicated AI Financing Team |\n",
       "\n",
       "## Conclusion\n",
       "Both NVIDIA and Goldman Sachs are navigating their respective markets with strategic initiatives aimed at capitalizing on the growth of AI. While NVIDIA leads in technology and innovation, Goldman Sachs leverages its financial expertise to support the burgeoning AI infrastructure sector. Investors should consider the strengths and challenges of each company, including potential risks such as competition and regulatory issues, as they evaluate investment opportunities in these dynamic sectors.\n",
       "\n",
       "## References\n",
       "- Financial reports from NVIDIA and Goldman Sachs\n",
       "- Market analysis from reputable financial news sources\n",
       "- Industry reports on AI technology and infrastructure financing\n",
       "\n",
       "By addressing the critiques and enhancing the clarity, objectivity, and completeness of this report, we provide a robust analysis of NVIDIA and Goldman Sachs, aiding investors in making informed decisions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: ðŸ’¾ Learning from the analysis...\n",
      "   - Memory added for GS\n",
      "   - Memory added for NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SyedM\\AppData\\Local\\Temp\\ipykernel_34924\\417482173.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
     ]
    }
   ],
   "source": [
    "# Define the research topic for the agent\n",
    "RESEARCH_TOPIC = \"Compare the recent performance and earnings of NVIDIA (NVDA) and Goldman Sachs (GS).\"\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = InvestmentResearchAgent()\n",
    "\n",
    "# Run the full research workflow\n",
    "agent.run(RESEARCH_TOPIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
